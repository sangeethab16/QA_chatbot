{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch import optim, nn\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "from torch.nn import functional as F\n",
    "import joblib\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import pickle\n",
    "import copy\n",
    "import traceback\n",
    "import math\n",
    "from torch.nn import init\n",
    "from torch.nn.utils import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefine variables\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "nll_sum = nn.CrossEntropyLoss(reduction = 'sum', ignore_index=IGNORE_INDEX)\n",
    "nll_average = nn.CrossEntropyLoss(reduction = 'mean', ignore_index=IGNORE_INDEX)\n",
    "nll_all = nn.CrossEntropyLoss(reduction = 'none', ignore_index=IGNORE_INDEX)\n",
    "\n",
    "word_mat = joblib.load('word_emb.pkl')\n",
    "char_mat = joblib.load('char_emb.pkl')\n",
    "with open('eval_examples_dev.json', \"r\") as fh:\n",
    "    dev_eval_file = json.load(fh)\n",
    "idx2word_dict = joblib.load('idx2word.pkl')\n",
    "\n",
    "batch_size = 64\n",
    "para_limit = 1000\n",
    "ques_limit = 80\n",
    "char_limit = 16\n",
    "sent_limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide data into batches\n",
    "\n",
    "class DataIterator(object):\n",
    "    def __init__(self, buckets, bsz, para_limit, ques_limit, char_limit, shuffle, sent_limit):\n",
    "        self.buckets = buckets\n",
    "        self.bsz = bsz\n",
    "        \n",
    "        # Fix class' para_limit and ques_limit\n",
    "        if para_limit is not None and ques_limit is not None:\n",
    "            self.para_limit = para_limit\n",
    "            self.ques_limit = ques_limit\n",
    "        else:\n",
    "            para_limit, ques_limit = 0, 0\n",
    "            for bucket in buckets:\n",
    "                for dp in bucket:\n",
    "                    para_limit = max(para_limit, dp['context_idxs'].size(0))\n",
    "                    ques_limit = max(ques_limit, dp['ques_idxs'].size(0))\n",
    "            self.para_limit, self.ques_limit = para_limit, ques_limit\n",
    "        self.char_limit = char_limit\n",
    "        self.sent_limit = sent_limit\n",
    "        \n",
    "        self.num_buckets = len(self.buckets)\n",
    "        \n",
    "        # Keep track of datapoints to choose from\n",
    "        self.bkt_pool = [i for i in range(self.num_buckets) if len(self.buckets[i]) > 0]\n",
    "        \n",
    "        # Shuffle the datapoints if shuffle = True\n",
    "        if shuffle:\n",
    "            for i in range(self.num_buckets):\n",
    "                random.shuffle(self.buckets[i])\n",
    "        self.bkt_ptrs = [0 for i in range(self.num_buckets)]\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        # Create empty tensors for each batch\n",
    "        context_idxs = torch.LongTensor(self.bsz, self.para_limit)\n",
    "        ques_idxs = torch.LongTensor(self.bsz, self.ques_limit)\n",
    "        context_char_idxs = torch.LongTensor(self.bsz, self.para_limit, self.char_limit)\n",
    "        ques_char_idxs = torch.LongTensor(self.bsz, self.ques_limit, self.char_limit)\n",
    "        y1 = torch.LongTensor(self.bsz)\n",
    "        y2 = torch.LongTensor(self.bsz)\n",
    "        q_type = torch.LongTensor(self.bsz)\n",
    "        start_mapping = torch.Tensor(self.bsz, self.para_limit, self.sent_limit)\n",
    "        end_mapping = torch.Tensor(self.bsz, self.para_limit, self.sent_limit)\n",
    "        all_mapping = torch.Tensor(self.bsz, self.para_limit, self.sent_limit)\n",
    "        is_support = torch.LongTensor(self.bsz, self.sent_limit)\n",
    "        \n",
    "        # Keep adding to batch till bucket pool has no elements\n",
    "        while True:\n",
    "            if len(self.bkt_pool) == 0: \n",
    "                break\n",
    "            \n",
    "            # Choose one data point from bucket pool\n",
    "            bkt_id = random.choice(self.bkt_pool) if self.shuffle else self.bkt_pool[0]\n",
    "            start_id = self.bkt_ptrs[bkt_id]\n",
    "            cur_bucket = self.buckets[bkt_id]\n",
    "            cur_bsz = min(self.bsz, len(cur_bucket) - start_id)\n",
    "\n",
    "            ids = []\n",
    "            \n",
    "            # Define current batch, sort according to size of context\n",
    "            cur_batch = cur_bucket[start_id: start_id + cur_bsz]\n",
    "            cur_batch.sort(key=lambda x: (x['context_idxs'] > 0).long().sum(), reverse=True)\n",
    "            \n",
    "            max_sent_cnt = 0\n",
    "            \n",
    "            # Fill tensors with 0 or otherwise\n",
    "            for mapping in [start_mapping, end_mapping, all_mapping]:\n",
    "                mapping.zero_()\n",
    "            is_support.fill_(IGNORE_INDEX)\n",
    "            \n",
    "            # Fill rest of the tensors by iterating over each datapoint in the batch\n",
    "            for i in range(len(cur_batch)):\n",
    "                context_idxs[i].copy_(cur_batch[i]['context_idxs'])\n",
    "                ques_idxs[i].copy_(cur_batch[i]['ques_idxs'])\n",
    "                context_char_idxs[i].copy_(cur_batch[i]['context_char_idxs'])\n",
    "                ques_char_idxs[i].copy_(cur_batch[i]['ques_char_idxs'])\n",
    "                \n",
    "                # Keep track of question types (0,1,2,3) based on y1 and y2\n",
    "                if cur_batch[i]['y1'] >= 0:\n",
    "                    y1[i] = cur_batch[i]['y1']\n",
    "                    y2[i] = cur_batch[i]['y2']\n",
    "                    q_type[i] = 0\n",
    "                elif cur_batch[i]['y1'] == -1:\n",
    "                    y1[i] = IGNORE_INDEX\n",
    "                    y2[i] = IGNORE_INDEX\n",
    "                    q_type[i] = 1\n",
    "                elif cur_batch[i]['y1'] == -2:\n",
    "                    y1[i] = IGNORE_INDEX\n",
    "                    y2[i] = IGNORE_INDEX\n",
    "                    q_type[i] = 2\n",
    "                elif cur_batch[i]['y1'] == -3:\n",
    "                    y1[i] = IGNORE_INDEX\n",
    "                    y2[i] = IGNORE_INDEX\n",
    "                    q_type[i] = 3\n",
    "                else:\n",
    "                    assert False\n",
    "                ids.append(cur_batch[i]['id'])\n",
    "                \n",
    "                # Fill start and end of each supporting facts, also if it is a supporting fact\n",
    "                for j, cur_sp_dp in enumerate(cur_batch[i]['start_end_facts']):\n",
    "                    if j >= self.sent_limit: break\n",
    "                    if len(cur_sp_dp) == 3:\n",
    "                        start, end, is_sp_flag = tuple(cur_sp_dp)\n",
    "                        end = min(end, para_limit)\n",
    "                    else:\n",
    "                        start, end, is_sp_flag, is_gold = tuple(cur_sp_dp)\n",
    "                        end = min(end, para_limit)\n",
    "                    if start < end:\n",
    "                        start_mapping[i, start, j] = 1\n",
    "                        end_mapping[i, end-1, j] = 1\n",
    "                        all_mapping[i, start:end, j] = 1\n",
    "                        is_support[i, j] = int(is_sp_flag)\n",
    "                max_sent_cnt = max(max_sent_cnt, len(cur_batch[i]['start_end_facts']))\n",
    "            \n",
    "            # Get max context length and question length to index the tensors\n",
    "            input_lengths = (context_idxs[:cur_bsz] > 0).long().sum(dim=1)\n",
    "            max_c_len = int(input_lengths.max())\n",
    "            max_q_len = int((ques_idxs[:cur_bsz] > 0).long().sum(dim=1).max())\n",
    "            \n",
    "            # Keep track of size of batch\n",
    "            self.bkt_ptrs[bkt_id] += cur_bsz\n",
    "            if self.bkt_ptrs[bkt_id] >= len(cur_bucket):\n",
    "                self.bkt_pool.remove(bkt_id)\n",
    "            \n",
    "            yield {'context_idxs': context_idxs[:cur_bsz, :max_c_len].contiguous(),\n",
    "                'ques_idxs': ques_idxs[:cur_bsz, :max_q_len].contiguous(),\n",
    "                'context_char_idxs': context_char_idxs[:cur_bsz, :max_c_len].contiguous(),\n",
    "                'ques_char_idxs': ques_char_idxs[:cur_bsz, :max_q_len].contiguous(),\n",
    "                'context_lens': input_lengths,\n",
    "                'y1': y1[:cur_bsz],\n",
    "                'y2': y2[:cur_bsz],\n",
    "                'ids': ids,\n",
    "                'q_type': q_type[:cur_bsz],\n",
    "                'is_support': is_support[:cur_bsz, :max_sent_cnt].contiguous(),\n",
    "                'start_mapping': start_mapping[:cur_bsz, :max_c_len, :max_sent_cnt],\n",
    "                'end_mapping': end_mapping[:cur_bsz, :max_c_len, :max_sent_cnt],\n",
    "                'all_mapping': all_mapping[:cur_bsz, :max_c_len, :max_sent_cnt]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LockedDropout masks a certain proportion of the input given a dropout probability ONLY during training\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropout = self.dropout\n",
    "        if not self.training:\n",
    "            return x\n",
    "        m = x.data.new(x.size(0), 1, x.size(2)).bernoulli_(1 - dropout)\n",
    "        mask = Variable(m.div_(1 - dropout), requires_grad=False)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "\n",
    "# Encoder RNN class\n",
    "class EncoderRNN(nn.Module):\n",
    "    # Initial input size, number of nodes in each layer, number of layers, Concat outputs bool, Bidirectional\n",
    "    # Encoder bool, dropout prob, returning for final datapoint bool\n",
    "    def __init__(self, input_size, num_units, nlayers, concat, bidir, dropout, return_last):\n",
    "        super().__init__()\n",
    "        self.rnns = []\n",
    "        for i in range(nlayers):\n",
    "            if i == 0: # First layer\n",
    "                input_size_ = input_size\n",
    "                output_size_ = num_units\n",
    "            else: # Other layers\n",
    "                input_size_ = num_units if not bidir else num_units * 2\n",
    "                output_size_ = num_units\n",
    "            self.rnns.append(nn.GRU(input_size_, output_size_, 1, bidirectional=bidir, batch_first=True))\n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        \n",
    "        # Keep track of weights\n",
    "        self.init_hidden = nn.ParameterList([nn.Parameter(torch.Tensor(2 if bidir else 1, 1, num_units).zero_()) for _ in range(nlayers)])\n",
    "        self.dropout = LockedDropout(dropout)\n",
    "        self.concat = concat\n",
    "        self.nlayers = nlayers\n",
    "        self.return_last = return_last\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for rnn in self.rnns:\n",
    "            for name, p in rnn.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    p.data.normal_(std=0.1)\n",
    "                else:\n",
    "                    p.data.zero_()\n",
    "\n",
    "    def get_init(self, bsz, i):\n",
    "        # Expand parameters to entire batch\n",
    "        return self.init_hidden[i].expand(-1, bsz, -1).contiguous()\n",
    "\n",
    "    def forward(self, input, input_lengths=None):\n",
    "        # Batch size and sequence length\n",
    "        bsz, slen = input.size(0), input.size(1)\n",
    "        output = input\n",
    "        outputs = []\n",
    "        if input_lengths is not None:\n",
    "            lens = input_lengths.data.cpu().numpy()\n",
    "        for i in range(self.nlayers):\n",
    "            # Get initial weights\n",
    "            hidden = self.get_init(bsz, i)\n",
    "            \n",
    "            # Apply dropout or mask a small % of output (in this case input)\n",
    "            output = self.dropout(output)\n",
    "            if input_lengths is not None:\n",
    "                # Pad input to max length\n",
    "                output = rnn.pack_padded_sequence(output, lens, batch_first=True)\n",
    "            # Apply the GRU to this unit\n",
    "            output, hidden = self.rnns[i](output, hidden)\n",
    "            if input_lengths is not None:\n",
    "                output, _ = rnn.pad_packed_sequence(output, batch_first=True)\n",
    "                if output.size(1) < slen: # used for parallel\n",
    "                    padding = Variable(output.data.new(1, 1, 1).zero_())\n",
    "                    output = torch.cat([output, padding.expand(output.size(0), slen-output.size(1), output.size(2))], dim=1)\n",
    "            if self.return_last:\n",
    "                outputs.append(hidden.permute(1, 0, 2).contiguous().view(bsz, -1))\n",
    "            else:\n",
    "                outputs.append(output)\n",
    "        if self.concat:\n",
    "            return torch.cat(outputs, dim=2)\n",
    "        # Return final output\n",
    "        return outputs[-1]\n",
    "\n",
    "class BiAttention(nn.Module):\n",
    "    def __init__(self, input_size, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = LockedDropout(dropout)\n",
    "        \n",
    "        # Linear transformations to input and memory\n",
    "        self.input_linear = nn.Linear(input_size, 1, bias=False)\n",
    "        self.memory_linear = nn.Linear(input_size, 1, bias=False)\n",
    "        \n",
    "        # Uniform attention initialized\n",
    "        self.dot_scale = nn.Parameter(torch.Tensor(input_size).uniform_(1.0 / (input_size ** 0.5)))\n",
    "\n",
    "    def forward(self, input, memory, mask):\n",
    "        bsz, input_len, memory_len = input.size(0), input.size(1), memory.size(1)\n",
    "\n",
    "        input = self.dropout(input)\n",
    "        memory = self.dropout(memory)\n",
    "        \n",
    "        # apply linear transformations\n",
    "        input_dot = self.input_linear(input)\n",
    "        memory_dot = self.memory_linear(memory).view(bsz, 1, memory_len)\n",
    "        \n",
    "        # Batch multiplication of inputs with memory without linear transformations\n",
    "        cross_dot = torch.bmm(input * self.dot_scale, memory.permute(0, 2, 1).contiguous())\n",
    "        \n",
    "        # Calculate attention as sum of matrix multiplications and linear transformations\n",
    "        att = input_dot + memory_dot + cross_dot\n",
    "        \n",
    "        # Don't pay attention to all tokens or padding\n",
    "        att = att - 1e30 * (1 - mask[:,None])\n",
    "        \n",
    "        # Attentions should sum to 1\n",
    "        weight_one = F.softmax(att, dim=-1)\n",
    "        output_one = torch.bmm(weight_one, memory)\n",
    "        weight_two = F.softmax(att.max(dim=-1)[0], dim=-1).view(bsz, 1, input_len)\n",
    "        output_two = torch.bmm(weight_two, input)\n",
    "\n",
    "        return torch.cat([input, output_one, input*output_one, output_two*output_one], dim=-1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, word_mat, char_mat):\n",
    "        super().__init__()\n",
    "        self.word_dim = 300\n",
    "        \n",
    "        # Convert to embeddings\n",
    "        self.word_emb = nn.Embedding(len(word_mat), len(word_mat[0]), padding_idx=0)\n",
    "        self.word_emb.weight.data.copy_(torch.from_numpy(word_mat))\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        self.char_emb = nn.Embedding(len(char_mat), len(char_mat[0]), padding_idx=0)\n",
    "        self.char_emb.weight.data.copy_(torch.from_numpy(char_mat))\n",
    "        \n",
    "        # 1D convolution definition\n",
    "        self.char_cnn = nn.Conv1d(8, 100, 5)\n",
    "        self.char_hidden = 100\n",
    "        self.hidden = 80\n",
    "        \n",
    "        # Building the architecture\n",
    "        self.rnn = EncoderRNN(100+self.word_dim, 80, 1, True, True, 0.2, False)\n",
    "\n",
    "        self.qc_att = BiAttention(80*2, 0.2)\n",
    "        self.linear_1 = nn.Sequential(\n",
    "                nn.Linear(80*8, 80),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.rnn_2 = EncoderRNN(80, 80, 1, False, True, 0.2, False)\n",
    "        self.self_att = BiAttention(80*2, 0.2)\n",
    "        self.linear_2 = nn.Sequential(\n",
    "                nn.Linear(80*8, 80),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.rnn_sp = EncoderRNN(80, 80, 1, False, True, 0.2, False)\n",
    "        self.linear_sp = nn.Linear(80*2, 1)\n",
    "\n",
    "        self.rnn_start = EncoderRNN(80+1, 80, 1, False, True, 0.2, False)\n",
    "        self.linear_start = nn.Linear(80*2, 1)\n",
    "\n",
    "        self.rnn_end = EncoderRNN(80*3+1, 80, 1, False, True, 0.2, False)\n",
    "        self.linear_end = nn.Linear(80*2, 1)\n",
    "\n",
    "        self.rnn_type = EncoderRNN(80*3+1, 80, 1, False, True, 0.2, False)\n",
    "        self.linear_type = nn.Linear(80*2, 3)\n",
    "\n",
    "        self.cache_S = 0\n",
    "    \n",
    "    # To mask the output, not see future tokens, only see last but 15 tokens\n",
    "    def get_output_mask(self, outer):\n",
    "        S = outer.size(1)\n",
    "        if S <= self.cache_S:\n",
    "            return Variable(self.cache_mask[:S, :S], requires_grad=False)\n",
    "        self.cache_S = S\n",
    "        np_mask = np.tril(np.triu(np.ones((S, S)), 0), 15)\n",
    "        self.cache_mask = outer.data.new(S, S).copy_(torch.from_numpy(np_mask))\n",
    "        return Variable(self.cache_mask, requires_grad=False)\n",
    "    \n",
    "    # Forward pass on output of DataIterator\n",
    "    def forward(self, context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, context_lens, start_mapping, end_mapping, all_mapping, return_yp=False):\n",
    "        para_size, ques_size, char_size, bsz = context_idxs.size(1), ques_idxs.size(1), context_char_idxs.size(2), context_idxs.size(0)\n",
    "        \n",
    "        # Number of datapoints to mask\n",
    "        context_mask = (context_idxs > 0).float()\n",
    "        ques_mask = (ques_idxs > 0).float()\n",
    "        \n",
    "        # Convert context and question to character embeddings\n",
    "        context_ch = self.char_emb(context_char_idxs.contiguous().view(-1, char_size)).view(bsz * para_size, char_size, -1)\n",
    "        ques_ch = self.char_emb(ques_char_idxs.contiguous().view(-1, char_size)).view(bsz * ques_size, char_size, -1)\n",
    "        \n",
    "        # CNN on character embeddings to get actual embeddings of characters from data and not the random input\n",
    "        context_ch = self.char_cnn(context_ch.permute(0, 2, 1).contiguous()).max(dim=-1)[0].view(bsz, para_size, -1)\n",
    "        ques_ch = self.char_cnn(ques_ch.permute(0, 2, 1).contiguous()).max(dim=-1)[0].view(bsz, ques_size, -1)\n",
    "        \n",
    "        # Word embeddings - not trained by model since we are already using glove\n",
    "        context_word = self.word_emb(context_idxs)\n",
    "        ques_word = self.word_emb(ques_idxs)\n",
    "        \n",
    "        # Concatenate word and character embeddings\n",
    "        context_output = torch.cat([context_word, context_ch], dim=2)\n",
    "        ques_output = torch.cat([ques_word, ques_ch], dim=2)\n",
    "        \n",
    "        # Run RNN on the embeddings\n",
    "        context_output = self.rnn(context_output, context_lens)\n",
    "        ques_output = self.rnn(ques_output)\n",
    "        \n",
    "        # Biattention model using RNN outputs\n",
    "        output = self.qc_att(context_output, ques_output, ques_mask)\n",
    "        output = self.linear_1(output)\n",
    "        \n",
    "        # RNN followed by self-attention model on context+ques\n",
    "        output_t = self.rnn_2(output, context_lens)\n",
    "        output_t = self.self_att(output_t, output_t, context_mask)\n",
    "        output_t = self.linear_2(output_t)\n",
    "        \n",
    "        # Add biattention and self-attention outputs\n",
    "        output = output + output_t\n",
    "        \n",
    "        # Predict if supporting fact using RNN\n",
    "        sp_output = self.rnn_sp(output, context_lens)\n",
    "\n",
    "        start_output = torch.matmul(start_mapping.permute(0, 2, 1).contiguous(), sp_output[:,:,self.hidden:])\n",
    "        end_output = torch.matmul(end_mapping.permute(0, 2, 1).contiguous(), sp_output[:,:,:self.hidden])\n",
    "        sp_output = torch.cat([start_output, end_output], dim=-1)\n",
    "        sp_output = self.linear_sp(sp_output)\n",
    "        sp_output_aux = Variable(sp_output.data.new(sp_output.size(0), sp_output.size(1), 1).zero_())\n",
    "        predict_support = torch.cat([sp_output_aux, sp_output], dim=-1).contiguous()\n",
    "        \n",
    "        # Use supporting fact prediction to predict the answers\n",
    "        sp_output = torch.matmul(all_mapping, sp_output)\n",
    "        output = torch.cat([output, sp_output], dim=-1)\n",
    "        \n",
    "        # Get span start and span end predictions for answer, first predict start and use that + previous output\n",
    "        # to predict end. Mask portions of outputs to avoid using future tokens for prediction\n",
    "        output_start = self.rnn_start(output, context_lens)\n",
    "        logit1 = self.linear_start(output_start).squeeze(2) - 1e30 * (1 - context_mask)\n",
    "        output_end = torch.cat([output, output_start], dim=2)\n",
    "        output_end = self.rnn_end(output_end, context_lens)\n",
    "        logit2 = self.linear_end(output_end).squeeze(2) - 1e30 * (1 - context_mask)\n",
    "        \n",
    "        # Predict answer type\n",
    "        output_type = torch.cat([output, output_end], dim=2)\n",
    "        output_type = torch.max(self.rnn_type(output_type, context_lens), 1)[0]\n",
    "        predict_type = self.linear_type(output_type)\n",
    "        \n",
    "        # Return the probabilities itself and not prediction\n",
    "        if not return_yp: return logit1, logit2, predict_type, predict_support\n",
    "\n",
    "        outer = logit1[:,:,None] + logit2[:,None]\n",
    "        outer_mask = self.get_output_mask(outer)\n",
    "        outer = outer - 1e30 * (1 - outer_mask[None].expand_as(outer))\n",
    "        yp1 = outer.max(dim=2)[0].max(dim=1)[1]\n",
    "        yp2 = outer.max(dim=1)[0].max(dim=1)[1]\n",
    "        return logit1, logit2, predict_type, predict_support, yp1, yp2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation functions\n",
    "\n",
    "def normalize_answer(s): # Some cleaning before comparing actual and predicted answers\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth): # Total correct / total predicted and total correct / actual tokens\n",
    "    # Harmonic mean of precision and recall = F1 score\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def exact_match_score(prediction, ground_truth): # Check if the two answers are exactly matching\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def convert_tokens(eval_file, qa_id, pp1, pp2, p_type): \n",
    "    # Get the predicted answers for each question ID in dict format\n",
    "    answer_dict = {}\n",
    "    for qid, p1, p2, type in zip(qa_id, pp1, pp2, p_type):\n",
    "        if type == 0:\n",
    "            context = eval_file[str(qid)][\"context\"]\n",
    "            spans = eval_file[str(qid)][\"spans\"]\n",
    "            start_idx = spans[p1][0]\n",
    "            end_idx = spans[p2][1]\n",
    "            answer_dict[str(qid)] = context[start_idx: end_idx]\n",
    "        elif type == 1:\n",
    "            answer_dict[str(qid)] = 'yes'\n",
    "        elif type == 2:\n",
    "            answer_dict[str(qid)] = 'no'\n",
    "        elif type == 3:\n",
    "            answer_dict[str(qid)] = 'noanswer'\n",
    "        else:\n",
    "            assert False\n",
    "    return answer_dict\n",
    "\n",
    "def evaluate(eval_file, answer_dict): # Get exact match anf F1 score\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in answer_dict.items():\n",
    "        total += 1\n",
    "        ground_truths = eval_file[key][\"answer\"]\n",
    "        prediction = value\n",
    "        assert len(ground_truths) == 1\n",
    "        cur_EM = exact_match_score(prediction, ground_truths[0])\n",
    "        cur_f1, _, _ = f1_score(prediction, ground_truths[0])\n",
    "        exact_match += cur_EM\n",
    "        f1 += cur_f1\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "# Get evaluation metrics for a batch\n",
    "def evaluate_batch(data_source, model, max_batches, eval_file):\n",
    "    answer_dict = {}\n",
    "    sp_dict = {}\n",
    "    total_loss, step_cnt = 0, 0\n",
    "    iter = data_source\n",
    "    for step, data in enumerate(iter):\n",
    "        # Check number of batches to calculate metrics for\n",
    "        if step >= max_batches and max_batches > 0: break\n",
    "\n",
    "        context_idxs = Variable(data['context_idxs'], volatile=True)\n",
    "        ques_idxs = Variable(data['ques_idxs'], volatile=True)\n",
    "        context_char_idxs = Variable(data['context_char_idxs'], volatile=True)\n",
    "        ques_char_idxs = Variable(data['ques_char_idxs'], volatile=True)\n",
    "        context_lens = Variable(data['context_lens'], volatile=True)\n",
    "        y1 = Variable(data['y1'], volatile=True)\n",
    "        y2 = Variable(data['y2'], volatile=True)\n",
    "        q_type = Variable(data['q_type'], volatile=True)\n",
    "        is_support = Variable(data['is_support'], volatile=True)\n",
    "        start_mapping = Variable(data['start_mapping'], volatile=True)\n",
    "        end_mapping = Variable(data['end_mapping'], volatile=True)\n",
    "        all_mapping = Variable(data['all_mapping'], volatile=True)\n",
    "\n",
    "        logit1, logit2, predict_type, predict_support, yp1, yp2 = model(context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, context_lens, start_mapping, end_mapping, all_mapping, return_yp=True)\n",
    "        loss = (nll_sum(predict_type, q_type) + nll_sum(logit1, y1) + nll_sum(logit2, y2)) / context_idxs.size(0) + config.sp_lambda * nll_average(predict_support.view(-1, 2), is_support.view(-1))\n",
    "        answer_dict_ = convert_tokens(eval_file, data['ids'], yp1.data.cpu().numpy().tolist(), yp2.data.cpu().numpy().tolist(), np.argmax(predict_type.data.cpu().numpy(), 1))\n",
    "        answer_dict.update(answer_dict_)\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "        step_cnt += 1\n",
    "    loss = total_loss / step_cnt\n",
    "    metrics = evaluate(eval_file, answer_dict)\n",
    "    metrics['loss'] = loss\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Make predictions and export them\n",
    "def predict(data_source, model, eval_file, prediction_file):\n",
    "    answer_dict = {}\n",
    "    sp_dict = {}\n",
    "    sp_th = 0.3\n",
    "    for step, data in enumerate(tqdm(data_source)):\n",
    "        context_idxs = Variable(data['context_idxs'], volatile=True)\n",
    "        ques_idxs = Variable(data['ques_idxs'], volatile=True)\n",
    "        context_char_idxs = Variable(data['context_char_idxs'], volatile=True)\n",
    "        ques_char_idxs = Variable(data['ques_char_idxs'], volatile=True)\n",
    "        context_lens = Variable(data['context_lens'], volatile=True)\n",
    "        start_mapping = Variable(data['start_mapping'], volatile=True)\n",
    "        end_mapping = Variable(data['end_mapping'], volatile=True)\n",
    "        all_mapping = Variable(data['all_mapping'], volatile=True)\n",
    "\n",
    "        logit1, logit2, predict_type, predict_support, yp1, yp2 = model(context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, context_lens, start_mapping, end_mapping, all_mapping, return_yp=True)\n",
    "        answer_dict_ = convert_tokens(eval_file, data['ids'], yp1.data.cpu().numpy().tolist(), yp2.data.cpu().numpy().tolist(), np.argmax(predict_type.data.cpu().numpy(), 1))\n",
    "        answer_dict.update(answer_dict_)\n",
    "        \n",
    "        # Append all the predicted supporting facts for each ID\n",
    "        predict_support_np = torch.sigmoid(predict_support[:, :, 1]).data.cpu().numpy()\n",
    "        for i in range(predict_support_np.shape[0]):\n",
    "            cur_sp_pred = []\n",
    "            cur_id = data['ids'][i]\n",
    "            for j in range(predict_support_np.shape[1]):\n",
    "                if j >= len(eval_file[cur_id]['sent2title_ids']): break\n",
    "                if predict_support_np[i, j] > sp_th:\n",
    "                    cur_sp_pred.append(eval_file[cur_id]['sent2title_ids'][j])\n",
    "            sp_dict.update({cur_id: cur_sp_pred})\n",
    "    \n",
    "    # Export answer and supporting facts\n",
    "    prediction = {'answer': answer_dict, 'sp': sp_dict}\n",
    "    with open(prediction_file, 'w') as f:\n",
    "        json.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_buckets = [torch.load('train_record.pkl')]\n",
    "    dev_buckets = [torch.load('dev_record.pkl')]\n",
    "    \n",
    "    # We create functions to create the batches since we don't want them in memory all at once \n",
    "    def build_train_iterator():\n",
    "        return DataIterator(train_buckets, batch_size, para_limit, ques_limit, char_limit, True, sent_limit)\n",
    "\n",
    "    def build_dev_iterator():\n",
    "        return DataIterator(dev_buckets, batch_size, para_limit, ques_limit, char_limit, False, sent_limit)\n",
    "    print('Building Iterators Done')\n",
    "    \n",
    "    # Load the model\n",
    "    model = Model(np.array(word_mat), np.array(char_mat))\n",
    "    \n",
    "    # Parallelize and build model\n",
    "    ori_model = model\n",
    "    model = nn.DataParallel(ori_model)\n",
    "    \n",
    "    lr = 0.5 \n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.5)\n",
    "    cur_patience = 0\n",
    "    total_loss = 0\n",
    "    global_step = 0\n",
    "    best_dev_F1 = None\n",
    "    stop_train = False\n",
    "    start_time = time.time()\n",
    "    eval_start_time = time.time()\n",
    "    model.train()\n",
    "    print('Start training')\n",
    "    for epoch in range(100):\n",
    "        for data in build_train_iterator():\n",
    "            context_idxs = Variable(data['context_idxs'])\n",
    "            ques_idxs = Variable(data['ques_idxs'])\n",
    "            context_char_idxs = Variable(data['context_char_idxs'])\n",
    "            ques_char_idxs = Variable(data['ques_char_idxs'])\n",
    "            context_lens = Variable(data['context_lens'])\n",
    "            y1 = Variable(data['y1'])\n",
    "            y2 = Variable(data['y2'])\n",
    "            q_type = Variable(data['q_type'])\n",
    "            is_support = Variable(data['is_support'])\n",
    "            start_mapping = Variable(data['start_mapping'])\n",
    "            end_mapping = Variable(data['end_mapping'])\n",
    "            all_mapping = Variable(data['all_mapping'])\n",
    "            \n",
    "            # Get predictions and calculate negative log likelihood loss\n",
    "            logit1, logit2, predict_type, predict_support = model(context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, context_lens, start_mapping, end_mapping, all_mapping, return_yp=False)\n",
    "            loss_1 = (nll_sum(predict_type, q_type) + nll_sum(logit1, y1) + nll_sum(logit2, y2)) / context_idxs.size(0)\n",
    "            loss_2 = nll_average(predict_support.view(-1, 2), is_support.view(-1))\n",
    "            loss = loss_1 + 0 * loss_2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data[0]\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                cur_loss = total_loss / config.period\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | step {:6d} | lr {:05.5f} | ms/batch {:5.2f} | train loss {:8.3f}'.format(epoch, global_step, lr, elapsed*1000/100, cur_loss))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            if global_step % 1000 == 0:\n",
    "                model.eval()\n",
    "                # Predict on dev data and get evaluation metrics\n",
    "                metrics = evaluate_batch(build_dev_iterator(), model, 0, dev_eval_file, config)\n",
    "                model.train()\n",
    "\n",
    "                print('| eval {:6d} in epoch {:3d} | time: {:5.2f}s | dev loss {:8.3f} | EM {:.4f} | F1 {:.4f}'.format(global_step//1000,\n",
    "                    epoch, time.time()-eval_start_time, metrics['loss'], metrics['exact_match'], metrics['f1']))\n",
    "\n",
    "                eval_start_time = time.time()\n",
    "\n",
    "                dev_F1 = metrics['f1']\n",
    "                if best_dev_F1 is None or dev_F1 > best_dev_F1:\n",
    "                    best_dev_F1 = dev_F1\n",
    "                    torch.save(ori_model.state_dict(), os.path.join('HOTPOT', 'model.pt'))\n",
    "                    cur_patience = 0\n",
    "                else:\n",
    "                    cur_patience += 1\n",
    "                    if cur_patience >= 1:\n",
    "                        lr /= 2.0\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                        if lr < 0.5 * 1e-2:\n",
    "                            stop_train = True\n",
    "                            break\n",
    "                        cur_patience = 0\n",
    "        if stop_train: break\n",
    "    print('best_dev_F1 {}'.format(best_dev_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_split):\n",
    "    word_mat = joblib.load('word_emb.pkl')\n",
    "    char_mat = joblib.load('char_emb.pkl')\n",
    "    if data_split == 'dev':\n",
    "        with open('eval_examples_dev.json', \"r\") as fh:\n",
    "            dev_eval_file = json.load(fh)\n",
    "    else:\n",
    "        with open('eval_examples_test.json', 'r') as fh:\n",
    "            dev_eval_file = json.load(fh)\n",
    "    idx2word_dict = joblib.load('idx2word.pkl')\n",
    "\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    if data_split == 'dev':\n",
    "        dev_buckets = get_buckets('dev_record.pkl')\n",
    "        para_limit = 1000\n",
    "        ques_limit = 80\n",
    "    elif data_split == 'test':\n",
    "        para_limit = None\n",
    "        ques_limit = None\n",
    "        dev_buckets = get_buckets('test_record.pkl')\n",
    "\n",
    "    def build_dev_iterator():\n",
    "        return DataIterator(dev_buckets, 64, para_limit,\n",
    "            ques_limit, 16, False, 100)\n",
    "\n",
    "    model = Model(word_mat, char_mat)\n",
    "    ori_model = model\n",
    "    ori_model.load_state_dict(torch.load(os.path.join('HOTPOT', 'model.pt')))\n",
    "    model = nn.DataParallel(ori_model)\n",
    "\n",
    "    model.eval()\n",
    "    predict(build_dev_iterator(), model, dev_eval_file, config, data_split + '_predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
