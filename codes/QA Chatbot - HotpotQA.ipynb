{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import ipynb\n",
    "import import_ipynb\n",
    "import prepro\n",
    "import random\n",
    "import numpy as np\n",
    "from ipynb.fs.full.prepro import *\n",
    "import torch\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'hotpot_train_v1.1.json'\n",
    "dev_distractor_filename = 'hotpot_dev_distractor_v1.json'\n",
    "dev_filename = 'hotpot_dev_fullwiki_v1.json'\n",
    "test_filename = 'hotpot_test_fullwiki_v1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2195892\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings_dict = {}\n",
    "embedding_size = 300\n",
    "with open('glove.840B.300d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:len(values) - embedding_size])\n",
    "        vector = np.asarray(values[-embedding_size:], \"float32\")\n",
    "        glove_embeddings_dict[word] = vector\n",
    "print(len(glove_embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = joblib.load('train_sample.pkl')\n",
    "dd = json.load(open(dev_distractor_filename, 'r'))\n",
    "dev = json.load(open(dev_filename, 'r'))\n",
    "test = json.load(open(test_filename, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_types = []\n",
    "for d in range(len(train)):\n",
    "    qa_types.append(train[d]['type'])\n",
    "    \n",
    "print(set(qa_types))\n",
    "print({x : qa_types.count(x) for x in set(qa_types)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    \n",
    "    # Fill context if empty\n",
    "    if len(article['context']) == 0:\n",
    "        article['context'] = [['some random title', 'some random stuff']]\n",
    "    \n",
    "    # Convert supporting facts to set of tuples if present, else empty set\n",
    "    if 'supporting_facts' in article:\n",
    "        sp_set = set(list(map(tuple, article['supporting_facts'])))\n",
    "    else:\n",
    "        sp_set = set()\n",
    "        \n",
    "    # Create spans for the titles and supporting facts, keep track of total text in supporting facts\n",
    "    text_context, context_tokens, context_chars = '', [], []\n",
    "    offsets = []\n",
    "    flat_offsets = []\n",
    "    start_end_facts = []\n",
    "    sent2title_ids = []\n",
    "    \n",
    "    def _process(sent, is_sup_fact, is_title=False):\n",
    "        \n",
    "        nonlocal text_context, context_tokens, context_chars, offsets, start_end_facts, flat_offsets\n",
    "        N_chars = len(text_context) # Keep track of existing text\n",
    "\n",
    "        sent_tokens = word_tokenize(sent)\n",
    "        if is_title:\n",
    "            sent = '<t> {} </t>'.format(sent)\n",
    "            sent_tokens = ['<t>'] + sent_tokens + ['</t>']\n",
    "        sent_chars = [list(token) for token in sent_tokens]\n",
    "        sent_spans = convert_idx(sent, sent_tokens)\n",
    "\n",
    "        sent_spans = [[N_chars + e[0], N_chars + e[1]] for e in sent_spans] # add offset to start and end indices of words\n",
    "\n",
    "        text_context += sent # Context text\n",
    "        context_tokens.extend(sent_tokens) # Word tokenized\n",
    "        context_chars.extend(sent_chars) # Individual characters\n",
    "        start_end_facts.append((len(context_tokens), len(context_tokens) + len(sent_tokens), is_sup_fact)) # Keep track of start and end of context\n",
    "        offsets.append(sent_spans) # Keep track of spans - position of words\n",
    "        flat_offsets.extend(sent_spans) # Keep track of spans - position of words\n",
    "    \n",
    "    # Count number of supporting facts per article\n",
    "    sp_fact_cnt = 0\n",
    "    for para in article['context']:\n",
    "        cur_title, cur_para = para[0], para[1]\n",
    "        _process(prepro_sent(cur_title), False, is_title=True)\n",
    "        sent2title_ids.append((cur_title, -1)) # Titles have index -1, 0 starts from supporting facts\n",
    "        for sent_id, sent in enumerate(cur_para):\n",
    "            is_sup_fact = (cur_title, sent_id) in sp_set\n",
    "            if is_sup_fact:\n",
    "                sp_fact_cnt += 1\n",
    "            _process(prepro_sent(sent), is_sup_fact)\n",
    "            sent2title_ids.append((cur_title, sent_id))\n",
    "            \n",
    "    # Calculate best possible answer span\n",
    "    if 'answer' in article: # Answer can be 'yes', 'no' or an actual answer which may or may not be present in the text context\n",
    "        answer = article['answer'].strip()\n",
    "        # best_indices has the start and end index of answer, if present in context\n",
    "        \n",
    "        if answer.lower() == 'yes':\n",
    "                best_indices = [-1, -1]\n",
    "        elif answer.lower() == 'no':\n",
    "                best_indices = [-2, -2]\n",
    "        else:\n",
    "            if article['answer'].strip() not in ''.join(text_context): \n",
    "                best_indices = (0, 1)\n",
    "            else:\n",
    "                _, best_indices, _ = fix_span(text_context, offsets, article['answer']) # Find location of answer in context\n",
    "                answer_span = []\n",
    "                for idx, span in enumerate(flat_offsets):\n",
    "                    if not (best_indices[1] <= span[0] or best_indices[0] >= span[1]):\n",
    "                        answer_span.append(idx)\n",
    "                best_indices = (answer_span[0], answer_span[-1]) # Get start and end indices of best possible answer\n",
    "    \n",
    "    else:\n",
    "        # If answer not present in article\n",
    "        answer = 'random'\n",
    "        best_indices = (0, 1)\n",
    "\n",
    "    ques_tokens = word_tokenize(article['question'])\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    example = {'context_tokens': context_tokens,\n",
    "               'context_chars': context_chars, \n",
    "               'ques_tokens': ques_tokens, \n",
    "               'ques_chars': ques_chars, \n",
    "               'y1s': [best_indices[0]], \n",
    "               'y2s': [best_indices[1]], \n",
    "               'id': article['_id'], \n",
    "               'start_end_facts': start_end_facts}\n",
    "    eval_example = {'context': text_context, \n",
    "                    'spans': flat_offsets, \n",
    "                    'answer': [answer], \n",
    "                    'id': article['_id'],\n",
    "                    'sent2title_ids': sent2title_ids}\n",
    "    \n",
    "    return example, eval_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes each article in required dataset by applying the process_article function\n",
    "\n",
    "def process_data(data, word_counter = None, char_counter = None):\n",
    "    \n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "\n",
    "    #outputs = Parallel(n_jobs = -1, verbose=10)(delayed(process_article)(article) for article in data)\n",
    "    outputs = [process_article(article) for article in data]\n",
    "    \n",
    "    examples = [e[0] for e in outputs]\n",
    "    for _, e in outputs:\n",
    "        if e is not None:\n",
    "            eval_examples[e['id']] = e\n",
    "\n",
    "    # only count during training\n",
    "    if word_counter is not None and char_counter is not None:\n",
    "        for example in examples:\n",
    "            for token in example['ques_tokens'] + example['context_tokens']:\n",
    "                word_counter[token] += 1\n",
    "                for char in token:\n",
    "                    char_counter[char] += 1\n",
    "\n",
    "    random.shuffle(examples)\n",
    "    print(\"{} questions in total\".format(len(examples)))\n",
    "\n",
    "    return examples, eval_examples, word_counter, char_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word embeddings\n",
    "\n",
    "def get_embeddings(counter, data_type, emb_file, size, vec_size, token2idx_dict = None, limit = -1):\n",
    "    \n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    \n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    \n",
    "    if emb_file is None:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.01) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "    else:\n",
    "        ks = list(emb_file.keys())\n",
    "        reqd_elements = set(ks).intersection(set(filtered_elements))\n",
    "        for e in reqd_elements:\n",
    "            embedding_dict[e] = emb_file[e]\n",
    "    \n",
    "    del emb_file\n",
    "    \n",
    "    print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "        len(embedding_dict), len(filtered_elements), data_type))\n",
    "    \n",
    "    # Create embeddings for NULL and Out-of-Vocabulary\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(\n",
    "        embedding_dict.keys(), 2)} #if token2idx_dict is None else token2idx_dict\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = 1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n",
    "    print('Done')\n",
    "    idx2emb_dict = {idx: embedding_dict[token]\n",
    "                    for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    print('Done')\n",
    "    #idx2token_dict = {idx: token for token, idx in token2idx_dict.items()}\n",
    "    idx2token_dict = dict(zip(token2idx_dict.values(), token2idx_dict.keys()))\n",
    "\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to convert all paragraphs and questions into indexes form\n",
    "\n",
    "def build_features_examples(examples, data_type, out_file, word2idx_dict, char2idx_dict):\n",
    "    if data_type == 'test':\n",
    "        para_limit, ques_limit = 0, 0\n",
    "        for example in tqdm(examples):\n",
    "            para_limit = max(para_limit, len(example['context_tokens']))\n",
    "            ques_limit = max(ques_limit, len(example['ques_tokens']))\n",
    "    else:\n",
    "        para_limit = 1000\n",
    "        ques_limit = 80\n",
    "\n",
    "    char_limit = 16\n",
    "    \n",
    "    # To remove contexts which exceed length limit set \n",
    "    def filter_func(example):\n",
    "        return len(example[\"context_tokens\"]) > para_limit or len(example[\"ques_tokens\"]) > ques_limit\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    datapoints = []\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    for example in tqdm(examples):\n",
    "        total_ += 1\n",
    "        \n",
    "        # Filter the examples with respect to length\n",
    "        if filter_func(example):\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        \n",
    "        # Empty arrays to hold question / paragraph vectors\n",
    "        context_idxs = np.zeros(para_limit, dtype=np.int64)\n",
    "        context_char_idxs = np.zeros((para_limit, char_limit), dtype=np.int64)\n",
    "        ques_idxs = np.zeros(ques_limit, dtype=np.int64)\n",
    "        ques_char_idxs = np.zeros((ques_limit, char_limit), dtype=np.int64)\n",
    "        \n",
    "        # Get index of word\n",
    "        def _get_word(word):\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in word2idx_dict:\n",
    "                    return word2idx_dict[each]\n",
    "            return 1\n",
    "        \n",
    "        # Get index of character\n",
    "        def _get_char(char):\n",
    "            if char in char2idx_dict:\n",
    "                return char2idx_dict[char]\n",
    "            return 1\n",
    "        \n",
    "        # Fill the arrays\n",
    "        context_idxs[:len(example['context_tokens'])] = [_get_word(token) for token in example['context_tokens']]\n",
    "        ques_idxs[:len(example['ques_tokens'])] = [_get_word(token) for token in example['ques_tokens']]\n",
    "\n",
    "        for i, token in enumerate(example[\"context_chars\"]):\n",
    "            l = min(len(token), char_limit)\n",
    "            context_char_idxs[i, :l] = [_get_char(char) for char in token[:l]]\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_chars\"]):\n",
    "            l = min(len(token), char_limit)\n",
    "            ques_char_idxs[i, :l] = [_get_char(char) for char in token[:l]]\n",
    "        \n",
    "        # Get the start and end indexes of the answer\n",
    "        start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "        y1, y2 = start, end\n",
    "        \n",
    "        # Collate into one list - result: a list of dictionaries\n",
    "        datapoints.append({'context_idxs': torch.from_numpy(context_idxs),\n",
    "            'context_char_idxs': torch.from_numpy(context_char_idxs),\n",
    "            'ques_idxs': torch.from_numpy(ques_idxs),\n",
    "            'ques_char_idxs': torch.from_numpy(ques_char_idxs),\n",
    "            'y1': y1,\n",
    "            'y2': y2,\n",
    "            'id': example['id'],\n",
    "            'start_end_facts': example['start_end_facts']})\n",
    "    print(\"Build {} / {} instances of features in total\".format(total, total_))\n",
    "    torch.save(datapoints, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample train data\n",
    "#train = random.sample(train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "examples, eval_examples, word_counter, char_counter = process_data(random.sample(train, 1000), Counter(), Counter())\n",
    "#examples, eval_examples = process_data(random.sample(test,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_mat, word2idx_dict, idx2word_dict = get_embeddings(word_counter, \"word\", emb_file = glove_embeddings_dict,\n",
    "                                                size = int(2.2e6), vec_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_emb_mat, char2idx_dict, idx2char_dict = get_embeddings(\n",
    "            char_counter, \"char\", emb_file=None, size = 94, vec_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_features_examples(examples, 'train', 'train_record.pkl', word2idx_dict, char2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export all necessary files (embeddings, features, processed text)\n",
    "\n",
    "word_counter_train, char_counter_train = Counter(), Counter()\n",
    "\n",
    "examples_train, eval_examples_train, word_counter_train, char_counter_train = process_data(train, \n",
    "                                                                               word_counter_train, char_counter_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(train, 'train_sample.pkl')\n",
    "\n",
    "#with open('examples_train.json', \"w\") as fh:\n",
    "#    json.dump(examples_train, fh)\n",
    "\n",
    "#with open('eval_examples_train.json', \"w\") as fh:\n",
    "#    json.dump(eval_examples_train, fh)\n",
    "\n",
    "#with open('word_counter_train.json', \"w\") as fh:\n",
    "#    json.dump(word_counter_train, fh)\n",
    "\n",
    "#with open('char_counter_train.json', \"w\") as fh:\n",
    "#    json.dump(char_counter_train, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7405 questions in total\n",
      "Test Done\n",
      "7405 questions in total\n",
      "Dev Done\n"
     ]
    }
   ],
   "source": [
    "examples_test, eval_examples_test, word_counter_test, char_counter_test = process_data(test)\n",
    "print('Test Done')\n",
    "examples_dev, eval_examples_dev, word_counter_dev, char_counter_dev = process_data(dev)\n",
    "print('Dev Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples_test.json', \"w\") as fh:\n",
    "    json.dump(examples_test, fh)\n",
    "\n",
    "with open('eval_examples_test.json', \"w\") as fh:\n",
    "    json.dump(eval_examples_test, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples_dev.json', \"w\") as fh:\n",
    "    json.dump(examples_dev, fh)\n",
    "\n",
    "with open('eval_examples_dev.json', \"w\") as fh:\n",
    "    json.dump(eval_examples_dev, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter_train = json.load(open('word_counter_train.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n",
      "178725 / 241010 tokens have corresponding word embedding vector\n",
      "Done\n",
      "Done\n",
      "0.5842909812927246\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "word_emb_mat, word2idx_dict, idx2word_dict = get_embeddings(word_counter_train, \"word\", emb_file = glove_embeddings_dict,\n",
    "                                                size = int(2.2e6), vec_size = 300)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del glove_embeddings_dict\n",
    "#gc.collect()\n",
    "\n",
    "#joblib.dump(word_emb_mat, 'word_emb.pkl')\n",
    "#joblib.dump(word2idx_dict, 'word2idx.pkl')\n",
    "#joblib.dump(idx2word_dict, 'idx2word.pkl')\n",
    "print('Done')\n",
    "del word_emb_mat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter_train = json.load(open('char_counter_train.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating char embedding...\n",
      "3798 tokens have corresponding embedding vector\n",
      "3798 / 3798 tokens have corresponding char embedding vector\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "char_emb_mat, char2idx_dict, idx2char_dict = get_embeddings(\n",
    "            char_counter_train, \"char\", emb_file=None, size = 94, vec_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(char_emb_mat, 'char_emb.pkl')\n",
    "joblib.dump(char2idx_dict, 'char2idx.pkl')\n",
    "joblib.dump(idx2char_dict, 'idx2char.pkl')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 288/10000 [00:00<00:07, 1228.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1466.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build 3068 / 10000 instances of features in total\n"
     ]
    }
   ],
   "source": [
    "examples_train = json.load(open('examples_train.json', 'r'))\n",
    "build_features_examples(examples_train, 'train', 'train_record.pkl', word2idx_dict, char2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:00<00:00, 285708.70it/s]\n",
      "  1%|          | 52/7405 [00:00<00:32, 226.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:24<00:00, 296.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build 7405 / 7405 instances of features in total\n"
     ]
    }
   ],
   "source": [
    "examples_test = json.load(open('examples_test.json', 'r'))\n",
    "build_features_examples(examples_test, 'test', 'test_record.pkl', word2idx_dict, char2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 92/7405 [00:00<00:07, 914.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:04<00:00, 1558.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build 2038 / 7405 instances of features in total\n"
     ]
    }
   ],
   "source": [
    "examples_dev = json.load(open('examples_dev.json', 'r'))\n",
    "build_features_examples(examples_dev, 'dev', 'dev_record.pkl', word2idx_dict, char2idx_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
