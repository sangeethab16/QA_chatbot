{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import import_ipynb\n",
    "import prepro\n",
    "import random\n",
    "from prepro import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'hotpot_train_v1.1.json'\n",
    "dev_distractor_filename = 'hotpot_dev_distractor_v1.json'\n",
    "dev_filename = 'hotpot_dev_fullwiki_v1.json'\n",
    "test_filename = 'hotpot_test_fullwiki_v1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2195892\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings_dict = {}\n",
    "embedding_size = 300\n",
    "with open('glove.840B.300d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:len(values) - embedding_size])\n",
    "        vector = np.asarray(values[-embedding_size:], \"float32\")\n",
    "        glove_embeddings_dict[word] = vector\n",
    "print(len(glove_embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mascogos',\n",
       "  ['The Mascogos (also known as \"negros mascagos\") are an afrodescendant group in Coahuila, Mexico.',\n",
       "   ' Centered on the town of El Nacimiento in Múzquiz Municipality, the group are descendants of Black Seminoles escaping the threat of slavery in the United States.']],\n",
       " ['Dhoolpet',\n",
       "  ['Dhoolpet is one of the old suburbs in Hyderabad, India.',\n",
       "   ' It is part of the old city of Hyderabad.',\n",
       "   ' This place is inhabited by people who migrated from Uttar Pradesh during the Nizam rule.',\n",
       "   ' The Nizam helped these people settle in this area.',\n",
       "   ' The area is notorious for bootlegging and has witnessed attacks on policemen or excise department officials during raids.']],\n",
       " ['Seminole music',\n",
       "  ['Seminole music is the music of the Seminole people, an indigenous people of the Americas who formed in Florida in the 18th century.',\n",
       "   ' Today most live in Oklahoma, but a minority continue in Florida.',\n",
       "   ' They have three federally recognized tribes, and some people belong to bands outside those groups.',\n",
       "   ' Their traditional music includes extensive use of rattles, hand drums, water drums, and flutes.']],\n",
       " ['Seminole',\n",
       "  ['The Seminole are a Native American people originally from Florida.',\n",
       "   ' Today, they principally live in Oklahoma with a minority in Florida, and comprise three federally recognized tribes: the Seminole Tribe of Oklahoma, the Seminole Tribe of Florida, and Miccosukee Tribe of Indians of Florida, as well as independent groups.',\n",
       "   ' The Seminole nation emerged in a process of ethnogenesis from various Native American groups who settled in Florida in the 18th century, most significantly northern Muscogee (Creeks) from what is now Georgia and Alabama.',\n",
       "   ' The word \"Seminole\" is derived from the Creek word \"simanó-li\", which may be itself be derived from the Spanish word \"cimarrón\", menaning \"runaway\" or \"wild one\".']],\n",
       " ['Draining and development of the Everglades',\n",
       "  ['The history of draining and development of the Everglades dates back to the 19th century.',\n",
       "   \" During the Second Seminole War beginning in 1836, the United States military's mission was to seek out Seminole people in the Everglades and capture or kill them.\",\n",
       "   ' Those missions gave the military the opportunity to map land that seemed to frustrate and confound them at every turn.',\n",
       "   ' A national push for expansion and progress toward the latter part of the 19th century stimulated interest in draining the Everglades for agricultural use.',\n",
       "   ' According to historians, \"From the middle of the nineteenth century to the middle of the twentieth century, the United States went through a period in which wetland removal was not questioned.',\n",
       "   ' Indeed, it was considered the proper thing to do.\"']],\n",
       " ['Muscogee language',\n",
       "  ['The Muscogee language (Mvskoke in Muscogee), also known as Creek, Seminole, Maskókî or Muskogee, is a Muskogean language spoken by Muscogee (Creek) and Seminole people, primarily in the U.S. states of Oklahoma and Florida.']],\n",
       " ['Seminole County, Oklahoma',\n",
       "  ['Seminole County is a county located in the U.S. state of Oklahoma.',\n",
       "   ' As of the 2010 census, the population was 25,482.',\n",
       "   ' Its county seat is Wewoka.',\n",
       "   \" Before Oklahoma's admission as a state, the county was the entire small portion of Indian Territory allocated to the Seminole people, who were removed from Florida in the 1820s.\"]],\n",
       " ['Battle of Jupiter Inlet',\n",
       "  ['The Battle of Jupiter Inlet occurred on January 15, 1838, between the Seminole Indians - Seminole Negro and the United States Navy.',\n",
       "   ' This was the first of a series of battles led by the US Navy in the area, also referred to as the First Battle of Loxahatchee.',\n",
       "   ' The battle started when Lt. Levin Powell led an expedition of 200 soldiers, sailors and marines, down the east coast of Florida.',\n",
       "   ' The Navy spotted a trail alongside the Jupiter Inlet, and seventy-five men were landed to find the Seminole camp nearby.',\n",
       "   ' The camp was led by Sam Jones, Ar-pi-uck-i, the spiritual medicine and war chief of the Miccosukee and Seminole people during the war.',\n",
       "   ' The officers that advanced on the camp were outnumbered and ambushed.',\n",
       "   ' After a long fight, the US retreated with five men killed in action and about twenty others wounded.',\n",
       "   ' Ltn.',\n",
       "   ' Powell was one of those killed in battle.']],\n",
       " ['Seminole Nation of Oklahoma',\n",
       "  ['The Seminole Nation of Oklahoma is a federally recognized Native American tribe based in the U.S. state of Oklahoma.',\n",
       "   ' It is the largest of the three federally recognized Seminole governments, which include the Seminole Tribe of Florida and the Miccosukee Tribe of Indians of Florida.',\n",
       "   ' Its members are descendants of the 3,000 Seminoles who were forcibly removed from Florida to Indian Territory, along with 800 Black Seminoles, after the Second Seminole War.',\n",
       "   ' The Seminole Nation of Oklahoma is headquartered in Wewoka within Seminole County, Oklahoma.',\n",
       "   ' Of 18,800 enrolled tribal members, 13,533 live within the state of Oklahoma.',\n",
       "   ' The tribe began to revive its government in 1936 under the Indian Reorganization Act.',\n",
       "   ' While its reservation was originally larger, today the tribal jurisdictional area covers Seminole County, Oklahoma, within which it has a variety of properties.']],\n",
       " ['Black Seminoles',\n",
       "  ['The Black Seminoles are black Indians associated with the Seminole people in Florida and Oklahoma.',\n",
       "   ' They are the descendants of free blacks and of escaped slaves (called maroons) who allied with Seminole groups in Spanish Florida.',\n",
       "   ' Historically, the Black Seminoles lived mostly in distinct bands near the Native American Seminole.',\n",
       "   ' Some were held as slaves of particular Seminole leaders; but they had more freedom than did slaves held by whites in the South and by other Native American tribes, including the right to bear arms.']]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[100]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(train_filename, 'r'))\n",
    "dd = json.load(open(dev_distractor_filename, 'r'))\n",
    "dev = json.load(open(dev_filename, 'r'))\n",
    "test = json.load(open(test_filename, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['supporting_facts', 'level', 'question', 'context', 'answer', '_id', 'type'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bridge', 'comparison'}\n",
      "{'bridge': 72991, 'comparison': 17456}\n"
     ]
    }
   ],
   "source": [
    "qa_types = []\n",
    "for d in range(len(train)):\n",
    "    qa_types.append(train[d]['type'])\n",
    "    \n",
    "print(set(qa_types))\n",
    "print({x : qa_types.count(x) for x in set(qa_types)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    \n",
    "    # Fill context if empty\n",
    "    if len(article['context']) == 0:\n",
    "        article['context'] = [['some random title', 'some random stuff']]\n",
    "    \n",
    "    # Convert supporting facts to set of tuples if present, else empty set\n",
    "    if 'supporting_facts' in article:\n",
    "        sp_set = set(list(map(tuple, article['supporting_facts'])))\n",
    "    else:\n",
    "        sp_set = set()\n",
    "        \n",
    "    # Create spans for the titles and supporting facts, keep track of total text in supporting facts\n",
    "    text_context, context_tokens, context_chars = '', [], []\n",
    "    offsets = []\n",
    "    flat_offsets = []\n",
    "    start_end_facts = []\n",
    "    sent2title_ids = []\n",
    "    \n",
    "    def _process(sent, is_sup_fact, is_title=False):\n",
    "        \n",
    "        nonlocal text_context, context_tokens, context_chars, offsets, start_end_facts, flat_offsets\n",
    "        N_chars = len(text_context) # Keep track of existing text\n",
    "\n",
    "        sent_tokens = word_tokenize(sent)\n",
    "        if is_title:\n",
    "            sent = '<t> {} </t>'.format(sent)\n",
    "            sent_tokens = ['<t>'] + sent_tokens + ['</t>']\n",
    "        sent_chars = [list(token) for token in sent_tokens]\n",
    "        sent_spans = convert_idx(sent, sent_tokens)\n",
    "\n",
    "        sent_spans = [[N_chars + e[0], N_chars + e[1]] for e in sent_spans] # add offset to start and end indices of words\n",
    "\n",
    "        text_context += sent # Context text\n",
    "        context_tokens.extend(sent_tokens) # Word tokenized\n",
    "        context_chars.extend(sent_chars) # Individual characters\n",
    "        start_end_facts.append((len(context_tokens), len(context_tokens) + len(sent_tokens), is_sup_fact)) # Keep track of start and end of context\n",
    "        offsets.append(sent_spans) # Keep track of spans - position of words\n",
    "        flat_offsets.extend(sent_spans) # Keep track of spans - position of words\n",
    "    \n",
    "    # Count number of supporting facts per article\n",
    "    sp_fact_cnt = 0\n",
    "    for para in article['context']:\n",
    "        cur_title, cur_para = para[0], para[1]\n",
    "        _process(prepro_sent(cur_title), False, is_title=True)\n",
    "        sent2title_ids.append((cur_title, -1)) # Titles have index -1, 0 starts from supporting facts\n",
    "        for sent_id, sent in enumerate(cur_para):\n",
    "            is_sup_fact = (cur_title, sent_id) in sp_set\n",
    "            if is_sup_fact:\n",
    "                sp_fact_cnt += 1\n",
    "            _process(prepro_sent(sent), is_sup_fact)\n",
    "            sent2title_ids.append((cur_title, sent_id))\n",
    "            \n",
    "    # Calculate best possible answer span\n",
    "    if 'answer' in article: # Answer can be 'yes', 'no' or an actual answer which may or may not be present in the text context\n",
    "        answer = article['answer'].strip()\n",
    "        # best_indices has the start and end index of answer, if present in context\n",
    "        \n",
    "        if answer.lower() == 'yes':\n",
    "                best_indices = [-1, -1]\n",
    "        elif answer.lower() == 'no':\n",
    "                best_indices = [-2, -2]\n",
    "        else:\n",
    "            if article['answer'].strip() not in ''.join(text_context): \n",
    "                best_indices = (0, 1)\n",
    "            else:\n",
    "                _, best_indices, _ = fix_span(text_context, offsets, article['answer']) # Find location of answer in context\n",
    "                answer_span = []\n",
    "                for idx, span in enumerate(flat_offsets):\n",
    "                    if not (best_indices[1] <= span[0] or best_indices[0] >= span[1]):\n",
    "                        answer_span.append(idx)\n",
    "                best_indices = (answer_span[0], answer_span[-1]) # Get start and end indices of best possible answer\n",
    "    \n",
    "    else:\n",
    "        # If answer not present in article\n",
    "        answer = 'random'\n",
    "        best_indices = (0, 1)\n",
    "\n",
    "    ques_tokens = word_tokenize(article['question'])\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    example = {'context_tokens': context_tokens,\n",
    "               'context_chars': context_chars, \n",
    "               'ques_tokens': ques_tokens, \n",
    "               'ques_chars': ques_chars, \n",
    "               'y1s': [best_indices[0]], \n",
    "               'y2s': [best_indices[1]], \n",
    "               'id': article['_id'], \n",
    "               'start_end_facts': start_end_facts}\n",
    "    eval_example = {'context': text_context, \n",
    "                    'spans': flat_offsets, \n",
    "                    'answer': [answer], \n",
    "                    'id': article['_id'],\n",
    "                    'sent2title_ids': sent2title_ids}\n",
    "    \n",
    "    return example, eval_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes each article in required dataset by applying the process_article function\n",
    "\n",
    "def process_data(data, word_counter = None, char_counter = None):\n",
    "    \n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "\n",
    "    #outputs = Parallel(n_jobs=12, verbose=10)(delayed(process_article)(article) for article in data)\n",
    "    outputs = [process_article(article) for article in data]\n",
    "    \n",
    "    examples = [e[0] for e in outputs]\n",
    "    for _, e in outputs:\n",
    "        if e is not None:\n",
    "            eval_examples[e['id']] = e\n",
    "\n",
    "    # only count during training\n",
    "    if word_counter is not None and char_counter is not None:\n",
    "        for example in examples:\n",
    "            for token in example['ques_tokens'] + example['context_tokens']:\n",
    "                word_counter[token] += 1\n",
    "                for char in token:\n",
    "                    char_counter[char] += 1\n",
    "\n",
    "    random.shuffle(examples)\n",
    "    print(\"{} questions in total\".format(len(examples)))\n",
    "\n",
    "    return examples, eval_examples, word_counter, char_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word embeddings\n",
    "\n",
    "def get_embeddings(counter, data_type, emb_file, size, vec_size, token2idx_dict = None, limit = -1):\n",
    "    \n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    \n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    \n",
    "    if emb_file is None:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.01) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "    else:\n",
    "        ks = list(emb_file.keys())\n",
    "        for e in filtered_elements:\n",
    "            if e in ks:\n",
    "                embedding_dict[e] = emb_file[e]\n",
    "    \n",
    "    print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "        len(embedding_dict), len(filtered_elements), data_type))\n",
    "    \n",
    "    # Create embeddings for NULL and Out-of-Vocabulary\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(\n",
    "        embedding_dict.keys(), 2)} if token2idx_dict is None else token2idx_dict\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = 1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n",
    "    idx2emb_dict = {idx: embedding_dict[token]\n",
    "                    for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "\n",
    "    idx2token_dict = {idx: token for token, idx in token2idx_dict.items()}\n",
    "\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to convert all paragraphs and questions into indexes form\n",
    "\n",
    "def build_features_examples(examples, data_type, out_file, word2idx_dict, char2idx_dict):\n",
    "    if data_type == 'test':\n",
    "        para_limit, ques_limit = 0, 0\n",
    "        for example in tqdm(examples):\n",
    "            para_limit = max(para_limit, len(example['context_tokens']))\n",
    "            ques_limit = max(ques_limit, len(example['ques_tokens']))\n",
    "    else:\n",
    "        para_limit = 1000\n",
    "        ques_limit = 80\n",
    "\n",
    "    char_limit = 16\n",
    "    \n",
    "    # To remove contexts which exceed length limit set \n",
    "    def filter_func(example):\n",
    "        return len(example[\"context_tokens\"]) > para_limit or len(example[\"ques_tokens\"]) > ques_limit\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    datapoints = []\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    for example in tqdm(examples):\n",
    "        total_ += 1\n",
    "        \n",
    "        # Filter the examples with respect to length\n",
    "        if filter_func(example):\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        \n",
    "        # Empty arrays to hold question / paragraph vectors\n",
    "        context_idxs = np.zeros(para_limit, dtype=np.int64)\n",
    "        context_char_idxs = np.zeros((para_limit, char_limit), dtype=np.int64)\n",
    "        ques_idxs = np.zeros(ques_limit, dtype=np.int64)\n",
    "        ques_char_idxs = np.zeros((ques_limit, char_limit), dtype=np.int64)\n",
    "        \n",
    "        # Get index of word\n",
    "        def _get_word(word):\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in word2idx_dict:\n",
    "                    return word2idx_dict[each]\n",
    "            return 1\n",
    "        \n",
    "        # Get index of character\n",
    "        def _get_char(char):\n",
    "            if char in char2idx_dict:\n",
    "                return char2idx_dict[char]\n",
    "            return 1\n",
    "        \n",
    "        # Fill the arrays\n",
    "        context_idxs[:len(example['context_tokens'])] = [_get_word(token) for token in example['context_tokens']]\n",
    "        ques_idxs[:len(example['ques_tokens'])] = [_get_word(token) for token in example['ques_tokens']]\n",
    "\n",
    "        for i, token in enumerate(example[\"context_chars\"]):\n",
    "            l = min(len(token), char_limit)\n",
    "            context_char_idxs[i, :l] = [_get_char(char) for char in token[:l]]\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_chars\"]):\n",
    "            l = min(len(token), char_limit)\n",
    "            ques_char_idxs[i, :l] = [_get_char(char) for char in token[:l]]\n",
    "        \n",
    "        # Get the start and end indexes of the answer\n",
    "        start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "        y1, y2 = start, end\n",
    "        \n",
    "        # Collate into one list - result: a list of dictionaries\n",
    "        datapoints.append({'context_idxs': torch.from_numpy(context_idxs),\n",
    "            'context_char_idxs': torch.from_numpy(context_char_idxs),\n",
    "            'ques_idxs': torch.from_numpy(ques_idxs),\n",
    "            'ques_char_idxs': torch.from_numpy(ques_char_idxs),\n",
    "            'y1': y1,\n",
    "            'y2': y2,\n",
    "            'id': example['id'],\n",
    "            'start_end_facts': example['start_end_facts']})\n",
    "    print(\"Build {} / {} instances of features in total\".format(total, total_))\n",
    "    torch.save(datapoints, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 questions in total\n"
     ]
    }
   ],
   "source": [
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "examples, eval_examples, word_counter, char_counter = process_data(random.sample(train, 1000), Counter(), Counter())\n",
    "#examples, eval_examples = process_data(random.sample(test,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n",
      "56903 / 64871 tokens have corresponding word embedding vector\n"
     ]
    }
   ],
   "source": [
    "word_emb_mat, word2idx_dict, idx2word_dict = get_embeddings(word_counter, \"word\", emb_file = glove_embeddings_dict,\n",
    "                                                size = int(2.2e6), vec_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating char embedding...\n",
      "1461 tokens have corresponding embedding vector\n",
      "1461 / 1461 tokens have corresponding char embedding vector\n"
     ]
    }
   ],
   "source": [
    "char_emb_mat, char2idx_dict, idx2char_dict = get_embeddings(\n",
    "            char_counter, \"char\", emb_file=None, size = 94, vec_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 2,\n",
       " 'h': 3,\n",
       " 'e': 4,\n",
       " 'O': 5,\n",
       " 'l': 6,\n",
       " 'd': 7,\n",
       " 'S': 8,\n",
       " 't': 9,\n",
       " 'y': 10,\n",
       " 'a': 11,\n",
       " 'o': 12,\n",
       " 'n': 13,\n",
       " 'N': 14,\n",
       " '.': 15,\n",
       " '1': 16,\n",
       " '0': 17,\n",
       " ',': 18,\n",
       " 'i': 19,\n",
       " 's': 20,\n",
       " 'c': 21,\n",
       " 'D': 22,\n",
       " 'w': 23,\n",
       " 'u': 24,\n",
       " 'k': 25,\n",
       " 'U': 26,\n",
       " 'f': 27,\n",
       " 'C': 28,\n",
       " '?': 29,\n",
       " '<': 30,\n",
       " '>': 31,\n",
       " 'L': 32,\n",
       " 'r': 33,\n",
       " 'v': 34,\n",
       " 'I': 35,\n",
       " 'R': 36,\n",
       " '/': 37,\n",
       " 'm': 38,\n",
       " 'W': 39,\n",
       " 'p': 40,\n",
       " 'b': 41,\n",
       " 'g': 42,\n",
       " 'A': 43,\n",
       " ' ': 44,\n",
       " 'x': 45,\n",
       " '6': 46,\n",
       " 'M': 47,\n",
       " 'G': 48,\n",
       " '4': 49,\n",
       " '8': 50,\n",
       " '2': 51,\n",
       " '-': 52,\n",
       " '(': 53,\n",
       " ':': 54,\n",
       " '\"': 55,\n",
       " 'á': 56,\n",
       " ';': 57,\n",
       " ')': 58,\n",
       " '7': 59,\n",
       " 'H': 60,\n",
       " 'ú': 61,\n",
       " 'z': 62,\n",
       " 'ŋ': 63,\n",
       " 'ȟ': 64,\n",
       " 'F': 65,\n",
       " 'P': 66,\n",
       " 'B': 67,\n",
       " '9': 68,\n",
       " '5': 69,\n",
       " 'K': 70,\n",
       " 'E': 71,\n",
       " \"'\": 72,\n",
       " 'q': 73,\n",
       " 'J': 74,\n",
       " 'V': 75,\n",
       " '3': 76,\n",
       " 'Y': 77,\n",
       " 'j': 78,\n",
       " 'X': 79,\n",
       " '&': 80,\n",
       " '#': 81,\n",
       " '–': 82,\n",
       " '—': 83,\n",
       " 'ü': 84,\n",
       " 'İ': 85,\n",
       " '’': 86,\n",
       " ']': 87,\n",
       " 'ı': 88,\n",
       " 'ğ': 89,\n",
       " 'Ç': 90,\n",
       " 'Z': 91,\n",
       " 'Ü': 92,\n",
       " '$': 93,\n",
       " '소': 94,\n",
       " '주': 95,\n",
       " '燒': 96,\n",
       " '酒': 97,\n",
       " '%': 98,\n",
       " 'í': 99,\n",
       " 'Þ': 100,\n",
       " 'ó': 101,\n",
       " 'ō': 102,\n",
       " 'ū': 103,\n",
       " '焼': 104,\n",
       " '酎': 105,\n",
       " 'Q': 106,\n",
       " 'ÿ': 107,\n",
       " 'é': 108,\n",
       " 'ã': 109,\n",
       " '“': 110,\n",
       " '”': 111,\n",
       " 'à': 112,\n",
       " 'î': 113,\n",
       " 'ô': 114,\n",
       " 'û': 115,\n",
       " 'ܫ': 116,\n",
       " 'ܠ': 117,\n",
       " 'ܡ': 118,\n",
       " 'ܐ': 119,\n",
       " 'س': 120,\n",
       " 'ل': 121,\n",
       " 'ي': 122,\n",
       " 'م': 123,\n",
       " 'ا': 124,\n",
       " 'ن': 125,\n",
       " '!': 126,\n",
       " 'ö': 127,\n",
       " 'ä': 128,\n",
       " 'ç': 129,\n",
       " 'و': 130,\n",
       " 'ى': 131,\n",
       " '\\u200e': 132,\n",
       " 'ệ': 133,\n",
       " 'ả': 134,\n",
       " 'ỹ': 135,\n",
       " 'ậ': 136,\n",
       " 'ự': 137,\n",
       " '院': 138,\n",
       " '寶': 139,\n",
       " '藏': 140,\n",
       " '美': 141,\n",
       " '術': 142,\n",
       " '越': 143,\n",
       " '南': 144,\n",
       " 'Š': 145,\n",
       " 'č': 146,\n",
       " 'ř': 147,\n",
       " '{': 148,\n",
       " '}': 149,\n",
       " '\\xa0': 150,\n",
       " 'ñ': 151,\n",
       " 'ā': 152,\n",
       " 'ス': 153,\n",
       " 'タ': 154,\n",
       " 'ー': 155,\n",
       " 'フ': 156,\n",
       " 'ォ': 157,\n",
       " 'ッ': 158,\n",
       " 'ク': 159,\n",
       " '輝': 160,\n",
       " 'き': 161,\n",
       " 'の': 162,\n",
       " 'ト': 163,\n",
       " '株': 164,\n",
       " '式': 165,\n",
       " '会': 166,\n",
       " '社': 167,\n",
       " 'ア': 168,\n",
       " '・': 169,\n",
       " 'エ': 170,\n",
       " 'ン': 171,\n",
       " 'テ': 172,\n",
       " 'イ': 173,\n",
       " 'メ': 174,\n",
       " 'Ā': 175,\n",
       " 'ß': 176,\n",
       " 'ò': 177,\n",
       " 'š': 178,\n",
       " '«': 179,\n",
       " 'Р': 180,\n",
       " 'о': 181,\n",
       " 'с': 182,\n",
       " 'и': 183,\n",
       " 'й': 184,\n",
       " 'к': 185,\n",
       " 'е': 186,\n",
       " 'ж': 187,\n",
       " 'н': 188,\n",
       " 'ф': 189,\n",
       " 'а': 190,\n",
       " 'ш': 191,\n",
       " 'т': 192,\n",
       " 'д': 193,\n",
       " 'в': 194,\n",
       " '»': 195,\n",
       " 'ș': 196,\n",
       " 'ț': 197,\n",
       " 'ă': 198,\n",
       " 'ش': 199,\n",
       " 'ر': 200,\n",
       " 'ف': 201,\n",
       " 'ة': 202,\n",
       " 'מ': 203,\n",
       " 'ו': 204,\n",
       " 'ש': 205,\n",
       " 'י': 206,\n",
       " 'ר': 207,\n",
       " 'פ': 208,\n",
       " 'ה': 209,\n",
       " 'Б': 210,\n",
       " '́': 211,\n",
       " 'л': 212,\n",
       " 'ь': 213,\n",
       " 'з': 214,\n",
       " 'п': 215,\n",
       " '€': 216,\n",
       " 'ù': 217,\n",
       " 'ư': 218,\n",
       " 'ớ': 219,\n",
       " 'â': 220,\n",
       " 'ạ': 221,\n",
       " 'ơ': 222,\n",
       " 'ì': 223,\n",
       " '²': 224,\n",
       " '[': 225,\n",
       " 'Ἄ': 226,\n",
       " 'δ': 227,\n",
       " 'ρ': 228,\n",
       " 'α': 229,\n",
       " 'σ': 230,\n",
       " 'τ': 231,\n",
       " 'ο': 232,\n",
       " 'ς': 233,\n",
       " 'ὁ': 234,\n",
       " 'Ἀ': 235,\n",
       " 'φ': 236,\n",
       " 'ι': 237,\n",
       " 'ε': 238,\n",
       " 'ύ': 239,\n",
       " 'Π': 240,\n",
       " 'ὶ': 241,\n",
       " 'Ἁ': 242,\n",
       " 'μ': 243,\n",
       " 'ν': 244,\n",
       " 'κ': 245,\n",
       " 'ῶ': 246,\n",
       " 'É': 247,\n",
       " 'ח': 248,\n",
       " 'ת': 249,\n",
       " 'נ': 250,\n",
       " 'צ': 251,\n",
       " 'æ': 252,\n",
       " 'Ḥ': 253,\n",
       " 'Ṛ': 254,\n",
       " '冷': 255,\n",
       " 'た': 256,\n",
       " 'い': 257,\n",
       " '熱': 258,\n",
       " '帯': 259,\n",
       " '魚': 260,\n",
       " 'գ': 261,\n",
       " 'ա': 262,\n",
       " 'մ': 263,\n",
       " 'փ': 264,\n",
       " 'ռ': 265,\n",
       " 'ṙ': 266,\n",
       " 'ë': 267,\n",
       " 'è': 268,\n",
       " 'أ': 269,\n",
       " 'ب': 270,\n",
       " 'د': 271,\n",
       " 'ح': 272,\n",
       " 'ʾ': 273,\n",
       " 'ī': 274,\n",
       " 'ḥ': 275,\n",
       " '\\u2009': 276,\n",
       " 'ṣ': 277,\n",
       " 'Ṣ': 278,\n",
       " '‐': 279,\n",
       " 'ʿ': 280,\n",
       " 'ع': 281,\n",
       " 'ت': 282,\n",
       " 'ʻ': 283,\n",
       " '+': 284,\n",
       " '£': 285,\n",
       " 'く': 286,\n",
       " 'る': 287,\n",
       " 'み': 288,\n",
       " '割': 289,\n",
       " 'り': 290,\n",
       " '人': 291,\n",
       " '形': 292,\n",
       " 'Μ': 293,\n",
       " 'έ': 294,\n",
       " 'γ': 295,\n",
       " 'Λ': 296,\n",
       " 'ά': 297,\n",
       " 'π': 298,\n",
       " 'β': 299,\n",
       " '近': 300,\n",
       " '代': 301,\n",
       " '格': 302,\n",
       " '制': 303,\n",
       " '度': 304,\n",
       " 'Á': 305,\n",
       " 'ಬ': 306,\n",
       " 'ೆ': 307,\n",
       " 'ಂ': 308,\n",
       " 'ಗ': 309,\n",
       " 'ಳ': 310,\n",
       " 'ೂ': 311,\n",
       " 'ರ': 312,\n",
       " 'ು': 313,\n",
       " 'ಉ': 314,\n",
       " 'ತ': 315,\n",
       " '್': 316,\n",
       " 'ಲ': 317,\n",
       " 'ೋ': 318,\n",
       " 'ಕ': 319,\n",
       " 'ಸ': 320,\n",
       " 'ಭ': 321,\n",
       " 'ಚ': 322,\n",
       " 'ನ': 323,\n",
       " 'ಾ': 324,\n",
       " 'ವ': 325,\n",
       " 'ಣ': 326,\n",
       " 'ಷ': 327,\n",
       " 'ೇ': 328,\n",
       " '渡': 329,\n",
       " '米': 330,\n",
       " '周': 331,\n",
       " '年': 332,\n",
       " '記': 333,\n",
       " '念': 334,\n",
       " '日': 335,\n",
       " '本': 336,\n",
       " '公': 337,\n",
       " '演': 338,\n",
       " '=': 339,\n",
       " '高': 340,\n",
       " '平': 341,\n",
       " 'ル': 342,\n",
       " '協': 343,\n",
       " '定': 344,\n",
       " '杭': 345,\n",
       " '州': 346,\n",
       " '派': 347,\n",
       " '迅': 348,\n",
       " '广': 349,\n",
       " '告': 350,\n",
       " '有': 351,\n",
       " '限': 352,\n",
       " '司': 353,\n",
       " 'ş': 354,\n",
       " 'ச': 355,\n",
       " 'ன': 356,\n",
       " '்': 357,\n",
       " 'ெ': 358,\n",
       " 'ய': 359,\n",
       " 'த': 360,\n",
       " 'ி': 361,\n",
       " 'க': 362,\n",
       " 'ள': 363,\n",
       " '₨': 364,\n",
       " 'ی': 365,\n",
       " 'خ': 366,\n",
       " 'ک': 367,\n",
       " 'ů': 368,\n",
       " '′': 369,\n",
       " '½': 370,\n",
       " 'ร': 371,\n",
       " 'ถ': 372,\n",
       " 'ไ': 373,\n",
       " 'ฟ': 374,\n",
       " '้': 375,\n",
       " 'า': 376,\n",
       " 'เ': 377,\n",
       " 'ฉ': 378,\n",
       " 'ล': 379,\n",
       " 'ิ': 380,\n",
       " 'ม': 381,\n",
       " 'พ': 382,\n",
       " 'ะ': 383,\n",
       " 'ก': 384,\n",
       " 'ี': 385,\n",
       " 'ย': 386,\n",
       " 'ต': 387,\n",
       " 'อ': 388,\n",
       " 'บ': 389,\n",
       " 'ช': 390,\n",
       " 'น': 391,\n",
       " 'ษ': 392,\n",
       " '‘': 393,\n",
       " 'ć': 394,\n",
       " 'А': 395,\n",
       " 'Ц': 396,\n",
       " '*': 397,\n",
       " 'ز': 398,\n",
       " 'ص': 399,\n",
       " 'র': 400,\n",
       " 'ু': 401,\n",
       " 'ম': 402,\n",
       " 'া': 403,\n",
       " 'গ': 404,\n",
       " 'হ': 405,\n",
       " 'ঠ': 406,\n",
       " 'ক': 407,\n",
       " 'ত': 408,\n",
       " 'О': 409,\n",
       " 'Ш': 410,\n",
       " 'ч': 411,\n",
       " 'р': 412,\n",
       " 'Ю': 413,\n",
       " 'ł': 414,\n",
       " 'ľ': 415,\n",
       " 'ý': 416,\n",
       " '六': 417,\n",
       " '勝': 418,\n",
       " '事': 419,\n",
       " '今': 420,\n",
       " '上': 421,\n",
       " '天': 422,\n",
       " '皇': 423,\n",
       " 'Ș': 424,\n",
       " 'گ': 425,\n",
       " 'ه': 426,\n",
       " 'Î': 427,\n",
       " 'В': 428,\n",
       " 'я': 429,\n",
       " 'М': 430,\n",
       " 'х': 431,\n",
       " 'К': 432,\n",
       " 'у': 433,\n",
       " 'м': 434,\n",
       " 'ц': 435,\n",
       " 'ы': 436,\n",
       " 'С': 437,\n",
       " 'Щ': 438,\n",
       " 'Д': 439,\n",
       " 'Г': 440,\n",
       " 'Н': 441,\n",
       " 'г': 442,\n",
       " 'ё': 443,\n",
       " 'Л': 444,\n",
       " 'ю': 445,\n",
       " 'Đ': 446,\n",
       " 'Ö': 447,\n",
       " '강': 448,\n",
       " '남': 449,\n",
       " '스': 450,\n",
       " '타': 451,\n",
       " '일': 452,\n",
       " 'œ': 453,\n",
       " 'ê': 454,\n",
       " 'б': 455,\n",
       " 'З': 456,\n",
       " 'ў': 457,\n",
       " '×': 458,\n",
       " '기': 459,\n",
       " '보': 460,\n",
       " '배': 461,\n",
       " 'Я': 462,\n",
       " 'ễ': 463,\n",
       " 'ỳ': 464,\n",
       " 'ỗ': 465,\n",
       " 'ق': 466,\n",
       " '広': 467,\n",
       " '島': 468,\n",
       " '市': 469,\n",
       " 'Զ': 470,\n",
       " 'ք': 471,\n",
       " 'ր': 472,\n",
       " 'ի': 473,\n",
       " 'Մ': 474,\n",
       " 'Պ': 475,\n",
       " 'ո': 476,\n",
       " 'ղ': 477,\n",
       " 'ս': 478,\n",
       " 'յ': 479,\n",
       " 'ն': 480,\n",
       " 'प': 481,\n",
       " 'ं': 482,\n",
       " 'क': 483,\n",
       " 'ज': 484,\n",
       " 'च': 485,\n",
       " 'ौ': 486,\n",
       " 'र': 487,\n",
       " 'ी': 488,\n",
       " 'パ': 489,\n",
       " 'ロ': 490,\n",
       " 'デ': 491,\n",
       " 'ィ': 492,\n",
       " 'ウ': 493,\n",
       " '悪': 494,\n",
       " '魔': 495,\n",
       " '城': 496,\n",
       " 'ド': 497,\n",
       " 'ラ': 498,\n",
       " 'キ': 499,\n",
       " 'ュ': 500,\n",
       " 'Ţ': 501,\n",
       " 'リ': 502,\n",
       " 'ァ': 503,\n",
       " 'レ': 504,\n",
       " 'ハ': 505,\n",
       " 'ツ': 506,\n",
       " 'ェ': 507,\n",
       " 'ペ': 508,\n",
       " 'シ': 509,\n",
       " 'カ': 510,\n",
       " '五': 511,\n",
       " '十': 512,\n",
       " '嵐': 513,\n",
       " '孝': 514,\n",
       " '…': 515,\n",
       " 'ï': 516,\n",
       " 'ť': 517,\n",
       " 'ရ': 518,\n",
       " 'ခ': 519,\n",
       " 'ိ': 520,\n",
       " 'ု': 521,\n",
       " 'င': 522,\n",
       " '်': 523,\n",
       " 'ယ': 524,\n",
       " 'ူ': 525,\n",
       " 'န': 526,\n",
       " 'က': 527,\n",
       " 'တ': 528,\n",
       " 'ဘ': 529,\n",
       " 'ေ': 530,\n",
       " 'ာ': 531,\n",
       " 'လ': 532,\n",
       " 'ံ': 533,\n",
       " 'း': 534,\n",
       " 'အ': 535,\n",
       " 'သ': 536,\n",
       " '石': 537,\n",
       " '破': 538,\n",
       " '茂': 539,\n",
       " 'Ś': 540,\n",
       " 'ę': 541,\n",
       " 'ż': 542,\n",
       " 'ą': 543,\n",
       " 'Ż': 544,\n",
       " 'ś': 545,\n",
       " 'ń': 546,\n",
       " 'И': 547,\n",
       " 'Ó': 548,\n",
       " 'å': 549,\n",
       " '@': 550,\n",
       " 'ह': 551,\n",
       " 'ु': 552,\n",
       " 'ा': 553,\n",
       " 'व': 554,\n",
       " 'द': 555,\n",
       " 'ळ': 556,\n",
       " 'े': 557,\n",
       " 'ð': 558,\n",
       " 'ニ': 559,\n",
       " '문': 560,\n",
       " '재': 561,\n",
       " '인': 562,\n",
       " '文': 563,\n",
       " '在': 564,\n",
       " '寅': 565,\n",
       " '恋': 566,\n",
       " 'ピ': 567,\n",
       " 'ד': 568,\n",
       " 'ג': 569,\n",
       " 'ל': 570,\n",
       " 'ø': 571,\n",
       " '菜': 572,\n",
       " '頭': 573,\n",
       " '好': 574,\n",
       " '彩': 575,\n",
       " 'ស': 576,\n",
       " '្': 577,\n",
       " 'រ': 578,\n",
       " 'ុ': 579,\n",
       " 'ក': 580,\n",
       " 'ជ': 581,\n",
       " 'ល': 582,\n",
       " 'គ': 583,\n",
       " 'ី': 584,\n",
       " '谭': 585,\n",
       " '凯': 586,\n",
       " '绿': 587,\n",
       " '色': 588,\n",
       " '观': 589,\n",
       " '察': 590,\n",
       " '녀': 591,\n",
       " '시': 592,\n",
       " '대': 593,\n",
       " '이': 594,\n",
       " '승': 595,\n",
       " '철': 596,\n",
       " '집': 597,\n",
       " '三': 598,\n",
       " '浦': 599,\n",
       " '知': 600,\n",
       " '良': 601,\n",
       " 'ਕ': 602,\n",
       " 'ੌ': 603,\n",
       " 'ਲ': 604,\n",
       " 'ਾ': 605,\n",
       " 'ਂ': 606,\n",
       " 'ē': 607,\n",
       " '蝶': 608,\n",
       " '野': 609,\n",
       " '正': 610,\n",
       " '洋': 611,\n",
       " '新': 612,\n",
       " '党': 613,\n",
       " '大': 614,\n",
       " '地': 615,\n",
       " 'ķ': 616,\n",
       " 'ב': 617,\n",
       " 'ז': 618,\n",
       " 'כ': 619,\n",
       " 'ן': 620,\n",
       " '谢': 621,\n",
       " '榛': 622,\n",
       " '－': 623,\n",
       " '苏': 624,\n",
       " '东': 625,\n",
       " '皋': 626,\n",
       " 'ব': 627,\n",
       " 'ন': 628,\n",
       " 'ল': 629,\n",
       " 'স': 630,\n",
       " 'ে': 631,\n",
       " 'П': 632,\n",
       " '\\u200c': 633,\n",
       " 'ग': 634,\n",
       " 'ठ': 635,\n",
       " 'ू': 636,\n",
       " 'ャ': 637,\n",
       " 'マ': 638,\n",
       " 'オ': 639,\n",
       " 'プ': 640,\n",
       " 'Ō': 641,\n",
       " '奇': 642,\n",
       " '金': 643,\n",
       " '獎': 644,\n",
       " '音': 645,\n",
       " '樂': 646,\n",
       " '劇': 647,\n",
       " '信': 648,\n",
       " '区': 649,\n",
       " '台': 650,\n",
       " '县': 651,\n",
       " 'ஏ': 652,\n",
       " 'வ': 653,\n",
       " 'எ': 654,\n",
       " 'ம': 655,\n",
       " 'ர': 656,\n",
       " 'ா': 657,\n",
       " 'ஜ': 658,\n",
       " 'Ñ': 659,\n",
       " 'ゲ': 660,\n",
       " 'ム': 661,\n",
       " 'ブ': 662,\n",
       " '女': 663,\n",
       " '穴': 664,\n",
       " '冒': 665,\n",
       " '険': 666,\n",
       " '♭': 667,\n",
       " '⁄': 668,\n",
       " '®': 669,\n",
       " '−': 670,\n",
       " '_': 671,\n",
       " 'ק': 672,\n",
       " 'א': 673,\n",
       " 'Е': 674,\n",
       " 'і': 675,\n",
       " 'ნ': 676,\n",
       " 'ო': 677,\n",
       " 'ე': 678,\n",
       " 'ა': 679,\n",
       " 'დ': 680,\n",
       " 'მ': 681,\n",
       " 'ი': 682,\n",
       " 'ვ': 683,\n",
       " 'ს': 684,\n",
       " 'ლ': 685,\n",
       " 'შ': 686,\n",
       " 'ძ': 687,\n",
       " 'კ': 688,\n",
       " 'ჭ': 689,\n",
       " 'ტ': 690,\n",
       " 'რ': 691,\n",
       " 'Ф': 692,\n",
       " 'У': 693,\n",
       " 'ї': 694,\n",
       " 'Č': 695,\n",
       " '橋': 696,\n",
       " '拓': 697,\n",
       " '☆': 698,\n",
       " 'غ': 699,\n",
       " 'ゆ': 700,\n",
       " '정': 701,\n",
       " '희': 702,\n",
       " '비': 703,\n",
       " 'ː': 704,\n",
       " '유': 705,\n",
       " '감': 706,\n",
       " '러': 707,\n",
       " '운': 708,\n",
       " '도': 709,\n",
       " 'ŭ': 710,\n",
       " 'ŏ': 711,\n",
       " '♢': 712,\n",
       " '法': 713,\n",
       " 'つ': 714,\n",
       " 'か': 715,\n",
       " '~': 716,\n",
       " '↑': 717,\n",
       " '♡': 718,\n",
       " '～': 719,\n",
       " 'ほ': 720,\n",
       " 'え': 721,\n",
       " 'に': 722,\n",
       " 'な': 723,\n",
       " 'モ': 724,\n",
       " 'し': 725,\n",
       " '使': 726,\n",
       " '方': 727,\n",
       " '速': 728,\n",
       " '水': 729,\n",
       " 'も': 730,\n",
       " 'こ': 731,\n",
       " 'ち': 732,\n",
       " '異': 733,\n",
       " '世': 734,\n",
       " '界': 735,\n",
       " 'は': 736,\n",
       " 'と': 737,\n",
       " '科': 738,\n",
       " '校': 739,\n",
       " '劣': 740,\n",
       " '等': 741,\n",
       " '生': 742,\n",
       " '芥': 743,\n",
       " '川': 744,\n",
       " '間': 745,\n",
       " '所': 746,\n",
       " '紗': 747,\n",
       " '織': 748,\n",
       " '早': 749,\n",
       " '見': 750,\n",
       " '沙': 751,\n",
       " '素': 752,\n",
       " '晴': 753,\n",
       " 'ら': 754,\n",
       " '祝': 755,\n",
       " '福': 756,\n",
       " 'を': 757,\n",
       " '！': 758,\n",
       " '佐': 759,\n",
       " '藤': 760,\n",
       " '勉': 761,\n",
       " '無': 762,\n",
       " '職': 763,\n",
       " '転': 764,\n",
       " '行': 765,\n",
       " 'っ': 766,\n",
       " '気': 767,\n",
       " 'だ': 768,\n",
       " 'す': 769,\n",
       " '宮': 770,\n",
       " '口': 771,\n",
       " '子': 772,\n",
       " '優': 773,\n",
       " '紮': 774,\n",
       " '師': 775,\n",
       " '兄': 776,\n",
       " '追': 777,\n",
       " '仔': 778,\n",
       " '逃': 779,\n",
       " '學': 780,\n",
       " '戰': 781,\n",
       " '警': 782,\n",
       " '蠟': 783,\n",
       " '筆': 784,\n",
       " '小': 785,\n",
       " '臭': 786,\n",
       " '屁': 787,\n",
       " '王': 788,\n",
       " 'म': 789,\n",
       " 'ट': 790,\n",
       " '्': 791,\n",
       " 'َ': 792,\n",
       " 'ك': 793,\n",
       " '樟': 794,\n",
       " '坊': 795,\n",
       " 'Т': 796,\n",
       " '足': 797,\n",
       " '袋': 798,\n",
       " 'ல': 799,\n",
       " 'ப': 800,\n",
       " 'ு': 801,\n",
       " 'घ': 802,\n",
       " 'ँ': 803,\n",
       " 'ल': 804,\n",
       " 'ण': 805,\n",
       " 'ॉ': 806,\n",
       " 'ो': 807,\n",
       " 'स': 808,\n",
       " 'त': 809,\n",
       " '`': 810,\n",
       " 'λ': 811,\n",
       " 'ή': 812,\n",
       " 'θ': 813,\n",
       " '二': 814,\n",
       " '哥': 815,\n",
       " '鄭': 816,\n",
       " '伊': 817,\n",
       " '麵': 818,\n",
       " 'ქ': 819,\n",
       " 'თ': 820,\n",
       " 'ც': 821,\n",
       " 'უ': 822,\n",
       " 'პ': 823,\n",
       " 'ბ': 824,\n",
       " 'გ': 825,\n",
       " 'ხ': 826,\n",
       " 'ส': 827,\n",
       " 'ว': 828,\n",
       " 'ำ': 829,\n",
       " '์': 830,\n",
       " 'ู': 831,\n",
       " '็': 832,\n",
       " 'ท': 833,\n",
       " 'ค': 834,\n",
       " 'โ': 835,\n",
       " 'ซ': 836,\n",
       " 'Β': 837,\n",
       " 'Ζ': 838,\n",
       " 'υ': 839,\n",
       " 'η': 840,\n",
       " '켄': 841,\n",
       " '지': 842,\n",
       " '김': 843,\n",
       " '男': 844,\n",
       " '한': 845,\n",
       " '솔': 846,\n",
       " '健': 847,\n",
       " '่': 848,\n",
       " 'ศ': 849,\n",
       " 'ุ': 850,\n",
       " 'ด': 851,\n",
       " 'ธ': 852,\n",
       " 'ပ': 853,\n",
       " 'မ': 854,\n",
       " 'ည': 855,\n",
       " '္': 856,\n",
       " '企': 857,\n",
       " '業': 858,\n",
       " '家': 859,\n",
       " '的': 860,\n",
       " '搖': 861,\n",
       " '籃': 862,\n",
       " 'ြ': 863,\n",
       " '³': 864,\n",
       " 'Κ': 865,\n",
       " 'ξ': 866,\n",
       " 'ห': 867,\n",
       " 'ั': 868,\n",
       " 'ื': 869,\n",
       " 'ภ': 870,\n",
       " 'ณ': 871,\n",
       " 'ง': 872,\n",
       " 'แ': 873,\n",
       " 'จ': 874,\n",
       " 'ฒ': 875,\n",
       " 'ึ': 876,\n",
       " 'ų': 877,\n",
       " 'Ἑ': 878,\n",
       " 'ゥ': 879,\n",
       " 'ネ': 880,\n",
       " 'ワ': 881,\n",
       " 'ミ': 882,\n",
       " 'ョ': 883,\n",
       " '零': 884,\n",
       " '〜': 885,\n",
       " 'Ş': 886,\n",
       " 'Å': 887,\n",
       " '잘': 888,\n",
       " '자': 889,\n",
       " '요': 890,\n",
       " '굿': 891,\n",
       " '나': 892,\n",
       " '잇': 893,\n",
       " 'お': 894,\n",
       " 'や': 895,\n",
       " 'ọ': 896,\n",
       " '|': 897,\n",
       " '∈': 898,\n",
       " 'ღ': 899,\n",
       " 'Ὕ': 900,\n",
       " 'Α': 901,\n",
       " 'ἰ': 902,\n",
       " 'ί': 903,\n",
       " 'Æ': 904,\n",
       " 'ω': 905,\n",
       " 'ὲ': 906,\n",
       " 'ὰ': 907,\n",
       " 'ἦ': 908,\n",
       " 'Τ': 909,\n",
       " 'ὸ': 910,\n",
       " 'Ῥ': 911,\n",
       " 'ῦ': 912,\n",
       " 'ἢ': 913,\n",
       " 'Σ': 914,\n",
       " 'ῆ': 915,\n",
       " 'ἱ': 916,\n",
       " 'ό': 917,\n",
       " '≡': 918,\n",
       " '°': 919,\n",
       " 'ब': 920,\n",
       " 'ड': 921,\n",
       " '़': 922,\n",
       " 'अ': 923,\n",
       " 'छ': 924,\n",
       " 'ै': 925,\n",
       " 'Δ': 926,\n",
       " 'Ν': 927,\n",
       " 'ņ': 928,\n",
       " 'ű': 929,\n",
       " 'ि': 930,\n",
       " 'य': 931,\n",
       " 'ष': 932,\n",
       " 'ध': 933,\n",
       " '名': 934,\n",
       " '古': 935,\n",
       " '屋': 936,\n",
       " '巽': 937,\n",
       " '真': 938,\n",
       " '悟': 939,\n",
       " '端': 940,\n",
       " '慎': 941,\n",
       " '吾': 942,\n",
       " '晋': 943,\n",
       " '津': 944,\n",
       " '臣': 945,\n",
       " '武': 946,\n",
       " '山': 947,\n",
       " 'इ': 948,\n",
       " 'ओ': 949,\n",
       " 'न': 950,\n",
       " '◇': 951,\n",
       " '◆': 952,\n",
       " '†': 953,\n",
       " 'À': 954,\n",
       " '•': 955,\n",
       " 'श': 956,\n",
       " 'Ú': 957,\n",
       " '匈': 958,\n",
       " '奴': 959,\n",
       " '葷': 960,\n",
       " '粥': 961,\n",
       " '獯': 962,\n",
       " '淳': 963,\n",
       " '維': 964,\n",
       " '施': 965,\n",
       " '餓': 966,\n",
       " '鬼': 967,\n",
       " '、': 968,\n",
       " '≤': 969,\n",
       " 'ј': 970,\n",
       " 'ظ': 971,\n",
       " 'Ä': 972,\n",
       " 'ポ': 973,\n",
       " 'ζ': 974,\n",
       " 'ঙ': 975,\n",
       " 'ি': 976,\n",
       " '্': 977,\n",
       " 'দ': 978,\n",
       " '∙': 979,\n",
       " '·': 980,\n",
       " 'ج': 981,\n",
       " 'ط': 982,\n",
       " 'إ': 983,\n",
       " 'ث': 984,\n",
       " '林': 985,\n",
       " '国': 986,\n",
       " '泰': 987,\n",
       " '映': 988,\n",
       " '画': 989,\n",
       " '蔵': 990,\n",
       " 'ズ': 991,\n",
       " '危': 992,\n",
       " '機': 993,\n",
       " '一': 994,\n",
       " '発': 995,\n",
       " 'ғ': 996,\n",
       " '直': 997,\n",
       " '紀': 998,\n",
       " '清': 999,\n",
       " '富': 1000,\n",
       " '士': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|██▋                                                                            | 34/1000 [00:00<00:02, 338.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|████▎                                                                          | 55/1000 [00:00<00:03, 284.29it/s]\u001b[A\n",
      "  9%|██████▉                                                                        | 88/1000 [00:00<00:03, 287.13it/s]\u001b[A\n",
      " 11%|████████▎                                                                     | 107/1000 [00:00<00:03, 236.33it/s]\u001b[A\n",
      " 13%|██████████▎                                                                   | 132/1000 [00:00<00:03, 238.03it/s]\u001b[A\n",
      " 15%|███████████▊                                                                  | 152/1000 [00:00<00:03, 217.51it/s]\u001b[A\n",
      " 19%|██████████████▌                                                               | 186/1000 [00:00<00:03, 240.99it/s]\u001b[A\n",
      " 22%|█████████████████▍                                                            | 224/1000 [00:00<00:02, 264.86it/s]\u001b[A\n",
      " 25%|███████████████████▌                                                          | 251/1000 [00:00<00:02, 255.30it/s]\u001b[A\n",
      " 28%|█████████████████████▉                                                        | 281/1000 [00:01<00:02, 259.40it/s]\u001b[A\n",
      " 31%|████████████████████████▎                                                     | 311/1000 [00:01<00:02, 260.52it/s]\u001b[A\n",
      " 34%|██████████████████████████▎                                                   | 338/1000 [00:01<00:02, 253.94it/s]\u001b[A\n",
      " 38%|█████████████████████████████▋                                                | 381/1000 [00:01<00:02, 286.02it/s]\u001b[A\n",
      " 42%|████████████████████████████████▋                                             | 419/1000 [00:01<00:01, 305.53it/s]\u001b[A\n",
      " 45%|███████████████████████████████████▏                                          | 451/1000 [00:01<00:02, 263.73it/s]\u001b[A\n",
      " 48%|█████████████████████████████████████▍                                        | 480/1000 [00:01<00:02, 244.73it/s]\u001b[A\n",
      " 51%|███████████████████████████████████████▌                                      | 508/1000 [00:01<00:01, 247.01it/s]\u001b[A\n",
      " 54%|█████████████████████████████████████████▉                                    | 538/1000 [00:02<00:01, 257.18it/s]\u001b[A\n",
      " 59%|█████████████████████████████████████████████▊                                | 587/1000 [00:02<00:01, 293.61it/s]\u001b[A\n",
      " 62%|████████████████████████████████████████████████▎                             | 619/1000 [00:02<00:01, 289.80it/s]\u001b[A\n",
      " 65%|██████████████████████████████████████████████████▋                           | 650/1000 [00:02<00:01, 285.48it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████▌                        | 686/1000 [00:02<00:01, 297.38it/s]\u001b[A\n",
      " 72%|███████████████████████████████████████████████████████▉                      | 717/1000 [00:02<00:00, 292.70it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████▊                  | 767/1000 [00:02<00:00, 330.76it/s]\u001b[A\n",
      " 83%|█████████████████████████████████████████████████████████████████             | 834/1000 [00:02<00:00, 380.38it/s]\u001b[A\n",
      " 88%|████████████████████████████████████████████████████████████████████▍         | 877/1000 [00:02<00:00, 356.85it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████▍      | 916/1000 [00:03<00:00, 344.32it/s]\u001b[A\n",
      " 95%|██████████████████████████████████████████████████████████████████████████▎   | 953/1000 [00:03<00:00, 334.15it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 299.49it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build 290 / 1000 instances of features in total\n"
     ]
    }
   ],
   "source": [
    "build_features_examples(examples, 'train', 'train_record.pkl', word2idx_dict, char2idx_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
